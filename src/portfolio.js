/* ============================================
   ðŸŽ¯ PORTFOLIO CONFIGURATION
   Professional Portfolio for Laxman K
   ============================================ */

// ============================================
// âš™ï¸ WEBSITE SETTINGS
// ============================================
const settings = {
  isSplash: true,
  useCustomCursor: false,
  useParticles: true,
  googleTrackingID: "UA-174238252-2",
};

// ============================================
// ðŸ  HOME PAGE - GREETING
// ============================================
const greeting = {
  title: "Hello World!",
  title2: "Laxman",
  logo_name: "Laxman.K",
  nickname: "laxman",
  full_name: "I'm Laxman",
  subTitle: ["AI/ML Engineer ðŸ¤–", "Full Stack Developer ðŸ’»"],
  subTitle2:
    "M.Tech & B.Tech in Computer Science from IIT Delhi. Building intelligent systems that scale.",
  resumeLink:
    "https://drive.google.com/file/d/1FLMddO9p9D9xJPYlBrpRJ47YXrisMBpy/view?usp=sharing",
  mail: "mailto:laxmankethavath5@gmail.com",
};

// ============================================
// ðŸ”— SOCIAL MEDIA LINKS
// ============================================
const socialMediaLinks = {
  github: "https://github.com/Laxman824",
  linkedin: "https://www.linkedin.com/in/k-laxman-44913a156/",
  gmail: "laxmankethavath5@gmail.com",
  twitter: "https://twitter.com/laxmankethavat2",
  instagram: "https://www.instagram.com/_laxman___lucky__/",
  facebook: "https://www.facebook.com/laxman.kethavath.96/",
  gitlab: "https://github.com/Laxman824",
  tel: "Tel:+9199999999",
  contactform:
    "https://us9.list-manage.com/contact-form?u=3d3df8c91e9dce3b0897e2347&form_id=5685bf117664ef066f78acb001c0c4c8",
};

// ============================================
// ðŸ’¼ SKILLS SECTION
// ============================================
const skills = {
  data: [
    {
      title: "Full Stack Development",
      fileName: "FullStackImg",
      skills: [
        "âš¡ Building responsive, high-performance web applications with React & Node.js",
        "âš¡ Developing RESTful APIs and microservices architecture",
        "âš¡ Database design and optimization (SQL & NoSQL)",
        "âš¡ Real-time applications with WebSockets and event-driven systems",
      ],
      softwareSkills: [
        {
          skillName: "React",
          fontAwesomeClassname: "logos:react",
          style: { color: "#61DAFB" },
        },
        {
          skillName: "TypeScript",
          fontAwesomeClassname: "logos:typescript-icon",
          style: { color: "#3178C6" },
        },
        {
          skillName: "JavaScript",
          fontAwesomeClassname: "logos:javascript",
          style: { color: "#F7DF1E" },
        },
        {
          skillName: "Node.js",
          fontAwesomeClassname: "logos:nodejs-icon",
          style: { color: "#339933" },
        },
        {
          skillName: "Python",
          fontAwesomeClassname: "logos:python",
          style: { color: "#3776AB" },
        },
        {
          skillName: "Java",
          fontAwesomeClassname: "logos:java",
          style: { color: "#007396" },
        },
        {
          skillName: "Spring Boot",
          fontAwesomeClassname: "logos:spring-icon",
          style: { color: "#6DB33F" },
        },
        {
          skillName: "FastAPI",
          fontAwesomeClassname: "simple-icons:fastapi",
          style: { color: "#009688" },
        },
        {
          skillName: "Flask",
          fontAwesomeClassname: "simple-icons:flask",
          style: { color: "#000000" },
        },
        {
          skillName: "HTML5",
          fontAwesomeClassname: "logos:html-5",
          style: { color: "#E34F26" },
        },
        {
          skillName: "CSS3",
          fontAwesomeClassname: "logos:css-3",
          style: { color: "#1572B6" },
        },
        {
          skillName: "PostgreSQL",
          fontAwesomeClassname: "logos:postgresql",
          style: { color: "#336791" },
        },
        {
          skillName: "MongoDB",
          fontAwesomeClassname: "logos:mongodb-icon",
          style: { color: "#47A248" },
        },
        {
          skillName: "MySQL",
          fontAwesomeClassname: "logos:mysql",
          style: { color: "#4479A1" },
        },
        {
          skillName: "C++",
          fontAwesomeClassname: "logos:c-plusplus",
          style: { color: "#00599C" },
        },
        {
          skillName: "PHP",
          fontAwesomeClassname: "logos:php",
          style: { color: "#777BB4" },
        },
        {
          skillName: "Bootstrap",
          fontAwesomeClassname: "logos:bootstrap",
          style: { color: "#7952B3" },
        },
        {
          skillName: "TailwindCSS",
          fontAwesomeClassname: "logos:tailwindcss-icon",
          style: { color: "#06B6D4" },
        },
        {
          skillName: "Git",
          fontAwesomeClassname: "logos:git-icon",
          style: { color: "#F05032" },
        },
        {
          skillName: "GitHub",
          fontAwesomeClassname: "logos:github-icon",
          style: { color: "#181717" },
        },
      ],
    },
    {
      title: "Gen AI & Machine Learning",
      fileName: "DataScienceImg",
      skills: [
        "âš¡ Building production-ready GenAI applications with RAG pipelines",
        "âš¡ Developing multi-agent AI systems using LangChain & CrewAI",
        "âš¡ Computer Vision solutions with OpenCV and deep learning",
        "âš¡ Experience with 17+ AI/ML projects in production",
      ],
      softwareSkills: [
        {
          skillName: "Python",
          fontAwesomeClassname: "logos:python",
          style: { color: "#3776AB" },
        },
        {
          skillName: "CrewAI",
          fontAwesomeClassname: "fluent:bot-sparkle-24-regular",
          style: { color: "#FF6B6B" },
        },
        {
          skillName: "LlamaIndex",
          fontAwesomeClassname: "simple-icons:meta",
          style: { color: "#7B61FF" },
        },
        {
          skillName: "AutoGen",
          fontAwesomeClassname: "simple-icons:microsoft",
          style: { color: "#0078D4" },
        },
        {
          skillName: "TensorFlow",
          fontAwesomeClassname: "logos:tensorflow",
          style: { color: "#FF6F00" },
        },
        {
          skillName: "Keras",
          fontAwesomeClassname: "simple-icons:keras",
          style: { color: "#D00000" },
        },
        {
          skillName: "PyTorch",
          fontAwesomeClassname: "logos:pytorch-icon",
          style: { color: "#EE4C2C" },
        },
        {
          skillName: "OpenCV",
          fontAwesomeClassname: "logos:opencv",
          style: { color: "#5C3EE8" },
        },
        {
          skillName: "Pandas",
          fontAwesomeClassname: "simple-icons:pandas",
          style: { color: "#150458" },
        },
        {
          skillName: "NumPy",
          fontAwesomeClassname: "logos:numpy",
          style: { color: "#013243" },
        },
        {
          skillName: "Scikit-learn",
          fontAwesomeClassname: "simple-icons:scikitlearn",
          style: { color: "#F7931E" },
        },

        {
          skillName: "LangChain",
          fontAwesomeClassname: "simple-icons:langchain",
          style: { color: "#1C3C3C" },
        },
        {
          skillName: "Google Gemini",
          fontAwesomeClassname: "simple-icons:googlegemini",
          style: { color: "#8E7B2" },
        },
        {
          skillName: "vLLM",
          fontAwesomeClassname: "simple-icons:v",
          style: { color: "#646CFF" },
        },
        {
          skillName: "Hugging Face",
          fontAwesomeClassname: "simple-icons:huggingface",
          style: { color: "#FFD21E" },
        },
      ],
    },
    {
      title: "â˜ï¸Cloud & DevOps",
      fileName: "CloudInfraImg",
      skills: [
        "âš¡ Architecting scalable cloud solutions on GCP, AWS & Azure",
        "âš¡ Container orchestration with Docker & Kubernetes",
        "âš¡ Deploying deep learning models on cloud for mobile devices",
        "âš¡ Designed and implemented snapshotting APIs for virtual machines",
      ],
      softwareSkills: [
        {
          skillName: "Google Cloud",
          fontAwesomeClassname: "logos:google-cloud",
          style: { color: "#4285F4" },
        },
        {
          skillName: "AWS",
          fontAwesomeClassname: "logos:aws",
          style: { color: "#FF9900" },
        },
        {
          skillName: "Azure",
          fontAwesomeClassname: "logos:microsoft-azure",
          style: { color: "#0078D4" },
        },
        {
          skillName: "Docker",
          fontAwesomeClassname: "logos:docker-icon",
          style: { color: "#2496ED" },
        },
        {
          skillName: "Kubernetes",
          fontAwesomeClassname: "logos:kubernetes",
          style: { color: "#326CE5" },
        },
        {
          skillName: "Firebase",
          fontAwesomeClassname: "logos:firebase",
          style: { color: "#FFCA28" },
        },
        {
          skillName: "MongoDB",
          fontAwesomeClassname: "logos:mongodb-icon",
          style: { color: "#47A248" },
        },
        {
          skillName: "Heroku",
          fontAwesomeClassname: "logos:heroku-icon",
          style: { color: "#430098" },
        },
        {
          skillName: "Netlify",
          fontAwesomeClassname: "logos:netlify",
          style: { color: "#00C7B7" },
        },
        {
          skillName: "Vercel",
          fontAwesomeClassname: "logos:vercel-icon",
          style: { color: "#000000" },
        },
      ],
    },
  ],
};

// ============================================
// ðŸŽ“ EDUCATION
// ============================================
const degrees = {
  degrees: [
    {
      title: "ðŸŽ“ IIT Delhi - Masters",
      subtitle: "M.Tech in Computer Science & Engineering",
      logo_path: "readingImg.png",
      alt_name: "IITD",
      duration: "2022 - 2024",
      descriptions: [
        "âš¡ Advanced coursework in Machine Learning, Cloud Computing, Data Mining & Security",
        "âš¡ Research on AI-powered accessibility tools (RAVI Project)",
        "âš¡ Completed certifications in ML/DL, Web Development & Cloud Technologies",
      ],
      website_link: "https://www.iitd.ac.in/",
      color_code: "#4A90E2",
    },
    {
      title: "ðŸŽ“ IIT Delhi - Bachelors",
      subtitle: "B.Tech in Computer Science & Engineering",
      logo_path: "readingImg.png",
      alt_name: "IITD",
      duration: "2018 - 2022",
      descriptions: [
        "âš¡ Strong foundation in Data Structures, Algorithms, DBMS & Networking",
        "âš¡ Completed 20+ projects spanning web development, AI/ML & systems",
        "âš¡ Hands-on experience with NLP projects for real web-based applications",
      ],
      website_link: "https://www.iitd.ac.in/",
      color_code: "#F6B808",
    },
  ],
};

// ============================================
// ðŸ“œ CERTIFICATIONS
// ============================================
const certifications = {
  certifications: [
    {
      title: "Complete Data Science & Machine Learning Bootcamp",
      subtitle: "The App Brewery",
      logo_path: "TheAppBrewery-Code.png",
      certificate_link: "#",
      alt_name: "TheAppBrewery",
      color_code: "#47A048",
    },
    {
      title: "The Complete Web Development Bootcamp",
      subtitle: "The App Brewery",
      logo_path: "TheAppBrewery-Code.png",
      certificate_link: "#",
      alt_name: "TheAppBrewery",
      color_code: "#F6B808",
    },
    {
      title: "LMS Built using MERN STACK",
      subtitle: "E-Learning Platform",
      logo_path: "startupindialogo_new.png",
      certificate_link: "#",
      alt_name: "MERN Stack",
      color_code: "#8C151599",
    },
    {
      title: "AWS Solutions Architect",
      subtitle: "Amazon Web Services",
      logo_path: "AmazonWebservices_Logo.png",
      certificate_link:
        "https://www.coursera.org/account/accomplishments/verify/SA3RFZRW6JRW",
      alt_name: "AWS",
      color_code: "#1e70c1",
    },
    {
      title: "Goldman Sachs Virtual Intern",
      subtitle: "Software Engineering Track",
      logo_path: "goldman.png",
      certificate_link:
        "https://github.com/Laxman824/Virtual-Internships/tree/main/GoldmanSachs",
      alt_name: "Goldman Sachs",
      color_code: "#7AB8D8",
    },
    {
      title: "JP Morgan Chase Virtual Intern",
      subtitle: "Software Engineering Track",
      logo_path: "Morgan.png",
      certificate_link:
        "https://github.com/Laxman824/Virtual-Internships/tree/main/JPMorgan%20Chase",
      alt_name: "JP Morgan",
      color_code: "#0A2540",
    },
  ],
};

// ============================================
// ðŸ’¼ EXPERIENCE
// ============================================
const experience = {
  title: "Experience ðŸ“ˆ",
  subtitle: "Work, Internship and Volunteership",
  description:
    "From IIT Delhi research labs to production systems at CAMS, I've built AI-powered solutions processing millions of documents. Completed 34+ projects with expertise in Full Stack, AI/ML, and Cloud Architecture.",
  header_image_path: "experience.svg",
  sections: [
    {
      title: "Work",
      experiences: [
        {
          title: "Senior AI Engineer",
          company: "Think360 AI - CAMS",
          company_url: "https://www.camsonline.com/",
          logo_path: "Think_logo.png",
          duration: "July 2025 - Present",
          location: "Chennai, India",
          description:
            "Leading AI initiatives for mutual fund operations. Architecting enterprise-scale GenAI solutions and LLM-powered automation systems.",
          color: "#667eea",
        },
        {
          title: "Software Developer",
          company: "CAMS Mutual Funds",
          company_url: "https://www.camsonline.com/",
          logo_path: "cams.png",
          duration: "June 2024 - July 2025",
          location: "Chennai, India",
          description:
            "Built CamsLens - AI-powered RAG platform processing 80,000+ financial documents with 95% accuracy. Developed intelligent email classification systems processing 1,000+ emails daily using LLMs.",
          color: "#2962FF",
        },
        {
          title: "Software Engineer",
          company: "RAVI - IIT Delhi",
          company_url: "https://assistech.iitd.ac.in/",
          logo_path: "assist.png",
          duration: "Aug 2022 - May 2024",
          location: "New Delhi, India",
          description:
            "Developed AI-powered Reading Assistant for Visually Impaired students. Built computer vision systems for automated image descriptions and ALT text generation for STEM documents.",
          color: "#00C853",
        },
        {
          title: "Real Time Project",
          company: "IIT Delhi",
          company_url: "https://iitd.ac.in",
          logo_path: "readingImg.png",
          duration: "Aug 2022 - Nov 2022",
          location: "New Delhi, India",
          description:
            "Developed Driver Drowsiness Detection system using OpenCV and Raspberry Pi. Real-time monitoring with audio alerts for driver safety.",
          color: "#FF6B6B",
        },
        {
          title: "Software Developer Intern",
          company: "Eamvey Technologies",
          company_url: "#",
          logo_path: "freshhut.jpg",
          duration: "June 2018 - Aug 2018",
          location: "Remote",
          description:
            "Developed e-commerce web applications using MERN stack and PHP. Built responsive frontends and RESTful APIs.",
          color: "#FFA500",
        },
      ],
    },
    {
      title: "Internships",
      experiences: [
        {
          title: "Software Engineering Intern",
          company: "Walmart Global (Virtual)",
          company_url:
            "https://github.com/Laxman824/Virtual-Internships/tree/main/walmart",
          logo_path: "wall.png",
          duration: "Summer 2024",
          location: "Virtual",
          description:
            "Designed data structures for shipping optimization and database architecture for large-scale retail systems.",
          color: "#0071CE",
        },
        {
          title: "Software Engineering Intern",
          company: "Goldman Sachs (Virtual)",
          company_url:
            "https://github.com/Laxman824/Virtual-Internships/tree/main/GoldmanSachs",
          logo_path: "goldman.png",
          duration: "Summer 2024",
          location: "Virtual",
          description:
            "Completed security engineering simulation - password cracking and security assessment techniques.",
          color: "#7AB8D8",
        },
        {
          title: "Software Engineering Intern",
          company: "HP - Hewlett-Packard (Virtual)",
          company_url:
            "https://github.com/Laxman824/Virtual-Internships/tree/main/HP%20Intern",
          logo_path: "HP.png",
          duration: "Summer 2024",
          location: "Virtual",
          description:
            "Completed software engineering job simulation focused on RESTful APIs and testing methodologies.",
          color: "#0096D6",
        },
        {
          title: "Software Engineering Intern",
          company: "JP Morgan Chase (Virtual)",
          company_url:
            "https://github.com/Laxman824/Virtual-Internships/tree/main/JPMorgan%20Chase",
          logo_path: "jp.png",
          duration: "Summer 2024",
          location: "Virtual",
          description:
            "Built real-time stock price data visualization dashboard using JPMorgan's Perspective library.",
          color: "#0A2540",
        },
        {
          title: "Front-End Developer Intern",
          company: "Eamvey Technologies",
          company_url: "#",
          logo_path: "freshhut.jpg",
          duration: "Summer 2020",
          location: "Remote",
          description:
            "Developed e-commerce web application using MERN Stack and PHP with payment integration.",
          color: "#800000",
        },
      ],
    },
    {
      title: "Volunteerships",
      experiences: [
        {
          title: "Teaching Assistant - COL100",
          company: "IIT Delhi",
          company_url: "https://iitd.ac.in/",
          logo_path: "readingImg.png",
          duration: "2022",
          location: "New Delhi",
          description:
            "Mentored 300+ freshmen in Introduction to Computer Science. Conducted doubt sessions, graded assignments, and designed practice problems.",
          color: "#1E3A5F",
        },
      ],
    },
  ],
};

// ============================================
// ðŸš€ PROJECTS HEADER
// ============================================
const projectsHeader = {
  title: "Projects ðŸš€",
  description:
    "From AI-powered financial platforms to full-stack web applications, I build solutions that solve real problems. Each project showcases different aspects of my technical expertise. Click on any project to explore more â†’",
  avatar_image_path: "projects_image.svg",
};

// ============================================
// ðŸŽ¯ PROJECTS DATA (Fixed Duplicate IDs)
// ============================================
// const projects = {
//   data: [
//     // ========== AI/ML & CLOUD PROJECTS ==========
//     {
//       id: "1",
//       name: "ðŸ¤– CamsLens - AI Financial Insights",
//       url: "https://www.businesstoday.in/personal-finance/news/story/cams-bets-big-on-ai-and-cloud-to-power-next-phase-of-mutual-fund-growth-501945-2025-11-12",
//       description:
//         "Enterprise GenAI RAG platform processing 80,000+ financial documents with 95% accuracy. Features real-time RSS feed integration, semantic search APIs, and custom ranking algorithms.",
//       languages: [
//         { name: "Google Cloud", iconifyClass: "logos:google-cloud" },
//         { name: "Vertex AI", iconifyClass: "simple-icons:googlegemini" },
//         { name: "Cloud Run", iconifyClass: "logos:docker-icon" },
//         { name: "BigQuery", iconifyClass: "simple-icons:googlebigquery" },
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "AI/ML", iconifyClass: "carbon:machine-learning-model" },
//       ],
//     },
//     {
//       id: "2",
//       name: "ðŸŽ® NexGenGaming - E-Commerce Platform",
//       url: "https://gaming-ecommerce-fronted.vercel.app/",
//       description:
//         "Full-stack gaming e-commerce with real-time inventory, order tracking, and payment integration. Features admin dashboard, product management, and responsive design.",
//       languages: [
//         { name: "React", iconifyClass: "logos:react" },
//         { name: "Node.js", iconifyClass: "logos:nodejs-icon" },
//         { name: "MongoDB", iconifyClass: "logos:mongodb-icon" },
//         { name: "Express", iconifyClass: "simple-icons:express" },
//         { name: "Firebase", iconifyClass: "logos:firebase" },
//         { name: "TailwindCSS", iconifyClass: "logos:tailwindcss-icon" },
//         { name: "Vercel", iconifyClass: "logos:vercel-icon" },
//       ],
//     },
//     {
//       id: "3",
//       name: "ðŸ¢ RoomFlow - AI Booking System",
//       url: "https://workspace-booking-system.vercel.app/",
//       description:
//         "AI-assisted workspace booking platform with dynamic pricing, real-time conflict prevention, and admin analytics. Features natural-language booking powered by LLMs.",
//       languages: [
//         { name: "React", iconifyClass: "logos:react" },
//         { name: "TypeScript", iconifyClass: "logos:typescript-icon" },
//         { name: "Node.js", iconifyClass: "logos:nodejs-icon" },
//         { name: "Express", iconifyClass: "simple-icons:express" },
//         { name: "Vite", iconifyClass: "logos:vitejs" },
//         { name: "PostgreSQL", iconifyClass: "logos:postgresql" },
//         { name: "TailwindCSS", iconifyClass: "logos:tailwindcss-icon" },
//         { name: "Vercel", iconifyClass: "logos:vercel-icon" },
//       ],
//     },
//     {
//       id: "4",
//       name: "ðŸ¦ SEBI Debarred AI PAN Extraction",
//       url: "https://github.com/Laxman824/",
//       description:
//         "Cloud-native backend monitoring SEBI regulatory orders, extracting debarred PAN details using AI-powered document parsing. Event-driven architecture on Google Cloud.",
//       languages: [
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "Google Cloud", iconifyClass: "logos:google-cloud" },
//         { name: "Vertex AI", iconifyClass: "simple-icons:googlegemini" },
//         { name: "Firestore", iconifyClass: "logos:firebase" },
//         { name: "Pub/Sub", iconifyClass: "logos:google-cloud" },
//         { name: "Docker", iconifyClass: "logos:docker-icon" },
//       ],
//     },
//     {
//       id: "5",
//       name: "ðŸ“§ AI-Powered Email Automation",
//       url: "https://github.com/Laxman824/",
//       description:
//         "Intelligent microservice classifying financial emails with 95% accuracy. Extracts PAN, ARN codes using LLM zero-shot learning. Processes 1,000+ emails daily.",
//       languages: [
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "FastAPI", iconifyClass: "simple-icons:fastapi" },
//         { name: "LLM", iconifyClass: "simple-icons:openai" },
//         { name: "vLLM", iconifyClass: "simple-icons:openai" },
//         { name: "Docker", iconifyClass: "logos:docker-icon" },
//         { name: "REST API", iconifyClass: "mdi:api" },
//       ],
//     },
//     {
//       id: "6",
//       name: "ðŸ“ˆ IPO Monitoring System",
//       url: "https://ipo-monitor-gmp.streamlit.app/Dashboard",
//       description:
//         "Real-time IPO Grey Market Premium monitoring with automated notifications. Tracks upcoming IPOs, subscription data, and GMP trends for investment decisions.",
//       languages: [
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "Streamlit", iconifyClass: "simple-icons:streamlit" },
//         { name: "Pandas", iconifyClass: "simple-icons:pandas" },
//         { name: "Web Scraping", iconifyClass: "mdi:spider-web" },
//       ],
//     },
//     {
//       id: "7",
//       name: "ðŸ¤– Autonomous Multi-Agent Banking",
//       url: "https://github.com/laxman824",
//       description:
//         "Advanced agentic workflow with 7 specialized AI agents using CrewAI and Gemini 2.5. Features autonomous intent routing and inter-agent delegation for banking operations.",
//       languages: [
//         { name: "CrewAI", iconifyClass: "fluent:bot-sparkle-24-regular" },
//         { name: "LangChain", iconifyClass: "simple-icons:langchain" },
//         { name: "Vertex AI", iconifyClass: "simple-icons:googlegemini" },
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "Google Cloud", iconifyClass: "logos:google-cloud" },
//       ],
//     },
//     {
//       id: "8",
//       name: "ðŸ“„ RSS Document Downloader",
//       url: "https://rss-feed-reader-and-pdf-downloader.streamlit.app/",
//       description:
//         "Automated document downloading from RSS feeds. Monitors regulatory feeds, downloads PDFs, and organizes content for compliance research.",
//       languages: [
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "RSS", iconifyClass: "mdi:rss" },
//         { name: "PDF Processing", iconifyClass: "simple-icons:adobeacrobatreader" },
//         { name: "Streamlit", iconifyClass: "simple-icons:streamlit" },
//       ],
//     },
//     {
//       id: "9",
//       name: "ðŸ“¨ MSG to PDF Converter",
//       url: "https://msg-to-pdf-converter-cams.streamlit.app/",
//       description:
//         "Batch converter for Outlook .MSG files to PDF. Processed 20,000+ financial emails with attachments. Preserves formatting and embedded images.",
//       languages: [
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "Outlook", iconifyClass: "mdi:microsoft-outlook" },
//         { name: "WeasyPrint", iconifyClass: "mdi:printer" },
//         { name: "Streamlit", iconifyClass: "simple-icons:streamlit" },
//       ],
//     },
//     {
//       id: "10",
//       name: "ðŸ“ PDF Footnotes Processor",
//       url: "https://pdf-proceappr-app-jxybczrguea2egjbmrk6yh.streamlit.app/",
//       description:
//         "Automated footnotes and reference mapping for academic documents. Extracts citations and generates structured Excel sheets for research papers.",
//       languages: [
//         { name: "OpenCV", iconifyClass: "logos:opencv" },
//         { name: "OCR", iconifyClass: "mdi:ocr" },
//         { name: "PDF Processing", iconifyClass: "simple-icons:adobeacrobatreader" },
//         { name: "Streamlit", iconifyClass: "simple-icons:streamlit" },
//       ],
//     },
//     {
//       id: "11",
//       name: "ðŸ” OCR Field Extraction",
//       url: "https://ocr-field-extraction.streamlit.app/",
//       description:
//         "Automated extraction of PAN, Folio numbers from scanned documents using DocTR and custom post-processing. Deployed as Streamlit web application.",
//       languages: [
//         { name: "OpenCV", iconifyClass: "logos:opencv" },
//         { name: "DocTR", iconifyClass: "simple-icons:readme" },
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "Streamlit", iconifyClass: "simple-icons:streamlit" },
//       ],
//     },
//     {
//       id: "12",
//       name: "ðŸ’¹ Stock Exchange Platform",
//       url: "https://github.com/Laxman824/CAMS-StockEx",
//       description:
//         "Full-stack stock trading platform with Spring Boot backend. Features real-time price updates, portfolio management, and market analytics dashboard.",
//       languages: [
//         { name: "Spring Boot", iconifyClass: "logos:spring-icon" },
//         { name: "Java", iconifyClass: "logos:java" },
//         { name: "REST API", iconifyClass: "mdi:api" },
//         { name: "Database", iconifyClass: "mdi:database" },
//       ],
//     },
//     {
//       id: "13",
//       name: "ðŸ˜´ Driver Drowsiness Detection",
//       url: "https://github.com/Laxman824/Projects-Assignments/tree/main/Realtime%20driver%20drowsiness%20detection",
//       description:
//         "Real-time embedded system for driver safety using Raspberry Pi. Detects eye closure and yawning with OpenCV and dlib. Triggers audio alerts.",
//       languages: [
//         { name: "OpenCV", iconifyClass: "logos:opencv" },
//         { name: "dlib", iconifyClass: "simple-icons:cmake" },
//         { name: "Embedded C", iconifyClass: "logos:c" },
//         { name: "Raspberry Pi", iconifyClass: "logos:raspberry-pi" },
//       ],
//     },
//     {
//       id: "14",
//       name: "ðŸ‘ï¸ RAVI - Visual Assistant",
//       url: "https://github.com/Laxman824/RAVI-2022-2023-",
//       description:
//         "AI-powered Reading Assistant for Visually Impaired. Automates image descriptions and ALT text generation for STEM documents using computer vision.",
//       languages: [
//         { name: "Azure", iconifyClass: "logos:microsoft-azure" },
//         { name: "JavaScript", iconifyClass: "logos:javascript" },
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "Docker", iconifyClass: "logos:docker-icon" },
//       ],
//     },
//     {
//       id: "15",
//       name: "ðŸ“Š Table Recognition System",
//       url: "https://github.com/Laxman824/MTP2-work",
//       description:
//         "Deep learning system for borderless table detection in PDF documents. Improves accessibility for screen readers. Part of M.Tech thesis at IIT Delhi.",
//       languages: [
//         { name: "Deep Learning", iconifyClass: "carbon:machine-learning-model" },
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "MTL-Tabnet", iconifyClass: "mdi:table" },
//         { name: "PyTorch", iconifyClass: "logos:pytorch-icon" },
//       ],
//     },
//     {
//       id: "16",
//       name: "â™¿ RAVI - Accessible STEM Education",
//       url: "https://github.com/Laxman824/RAVI-2022-2023-",
//       description:
//         "AI-powered accessibility tool for STEM documents. Enables visually impaired students to understand scientific illustrations through intelligent audio descriptions.",
//       languages: [
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "Flask", iconifyClass: "simple-icons:flask" },
//         { name: "HTML", iconifyClass: "logos:html-5" },
//         { name: "Machine Learning", iconifyClass: "carbon:machine-learning-model" },
//         { name: "Computer Vision", iconifyClass: "mdi:eye-outline" },
//       ],
//     },
//     {
//       id: "17",
//       name: "âœˆï¸ SkyLine Airlines - DBMS System",
//       url: "https://github.com/Laxman824/Dbms-Project",
//       description:
//         "Airline reservation system with Flask and PostgreSQL. Features flight search, booking management, seat selection, and advanced SQL query optimization.",
//       languages: [
//         { name: "Flask", iconifyClass: "simple-icons:flask" },
//         { name: "PostgreSQL", iconifyClass: "logos:postgresql" },
//         { name: "HTML/CSS", iconifyClass: "logos:html-5" },
//         { name: "SQL", iconifyClass: "mdi:database" },
//       ],
//     },
//     {
//       id: "18",
//       name: "ðŸŒ BitTorrent File Transfer",
//       url: "https://github.com/Laxman824/Projects-Assignments/tree/main/BitTorrent-technique",
//       description:
//         "Implementation of BitTorrent P2P protocol with parallel TCP connections. Downloads chunks from multiple peers simultaneously and reconstructs files efficiently.",
//       languages: [
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "Networking", iconifyClass: "mdi:network" },
//         { name: "TCP/IP", iconifyClass: "carbon:network-3" },
//         { name: "P2P", iconifyClass: "mdi:share-variant" },
//       ],
//     },
//     {
//       id: "19",
//       name: "ðŸ’³ AmudalaKunta - Payment Gateway",
//       url: "https://laxman824.github.io/AmudalaKunta/",
//       description:
//         "Live web application with integrated payment gateway and admin dashboard. Secure transaction processing with real-time analytics.",
//       languages: [
//         { name: "HTML/CSS", iconifyClass: "logos:html-5" },
//         { name: "JavaScript", iconifyClass: "logos:javascript" },
//         { name: "Payment Gateway", iconifyClass: "mdi:credit-card-outline" },
//         { name: "Dashboard", iconifyClass: "mdi:view-dashboard" },
//       ],
//     },
//     {
//       id: "20",
//       name: "ðŸŽ® AI Pacman - Multi-Agent Search",
//       url: "https://github.com/Laxman824/Projects-Assignments/tree/main/AI-Pacman%20heuristics",
//       description:
//         "Advanced AI for Pacman featuring DFS, BFS, A*, and minimax algorithms. Implements multi-agent adversarial search with alpha-beta pruning.",
//       languages: [
//         { name: "Python", iconifyClass: "logos:python" },
//         { name: "AI/ML", iconifyClass: "carbon:machine-learning-model" },
//         { name: "Algorithms", iconifyClass: "carbon:flow" },
//         { name: "Game AI", iconifyClass: "mdi:gamepad-variant" },
//       ],
//     },
//     {
//       id: "21",
//       name: "ðŸ¢ Smart Lift Controller System",
//       url: "https://github.com/Laxman824/COL215-COMP-ARCH-assignments",
//       description:
//         "Intelligent elevator control system in Verilog. Implements SCAN algorithm, handles concurrent requests, and minimizes wait times for multi-floor buildings.",
//       languages: [
//         { name: "Verilog", iconifyClass: "simple-icons:v" },
//         { name: "Logic Design", iconifyClass: "mdi:chip" },
//         { name: "Simulation", iconifyClass: "mdi:elevator" },
//         { name: "Algorithms", iconifyClass: "carbon:flow-data" },
//       ],
//     },
//     {
//       id: "22",
//       name: "ðŸ’¾ VM Snapshot System",
//       url: "https://github.com/Laxman824/LiveSnapshotProject",
//       description:
//         "KVM-based VM snapshot service built with Rust. Features live snapshotting, incremental backups, and web interface for VM state management.",
//       languages: [
//         { name: "Rust", iconifyClass: "logos:rust" },
//         { name: "Virtualization", iconifyClass: "simple-icons:vmware" },
//         { name: "HTML", iconifyClass: "logos:html-5" },
//         { name: "CSS", iconifyClass: "logos:css-3" },
//       ],
//     },
//   ],
// };
const projects = {
  data: [
    // ==========================================
    // 1. CAMSLENS - AI FINANCIAL INSIGHTS
    // ==========================================
    {
      id: "1",
      name: "ðŸ¤– CamsLens - AI Financial Insights",
      url:
        "https://www.businesstoday.in/personal-finance/news/story/cams-bets-big-on-ai-and-cloud-to-power-next-phase-of-mutual-fund-growth-501945-2025-11-12",
      description:
        "Enterprise GenAI RAG platform processing 80,000+ financial documents with 95% accuracy and response in 7sec.",
      languages: [
        { name: "Google Cloud", iconifyClass: "logos:google-cloud" },
        { name: "Vertex AI", iconifyClass: "simple-icons:googlegemini" },
        { name: "Cloud Run", iconifyClass: "logos:docker-icon" },
        { name: "BigQuery", iconifyClass: "simple-icons:googlebigquery" },
        { name: "Python", iconifyClass: "logos:python" },
        { name: "AI/ML", iconifyClass: "carbon:machine-learning-model" },
      ],
      details: {
        problem:
          "Financial analysts at CAMS spent 4+ hours daily manually searching through 80,000+ regulatory documents, SEBI circulars, and compliance PDFs. Finding relevant information was slow, error-prone, and couldn't scale with growing regulatory requirements.",
        solution:
          "Built an AI-powered RAG (Retrieval Augmented Generation) platform that understands natural language queries and retrieves precise answers from the entire document corpus in seconds. Implemented semantic search with vector embeddings, custom ranking algorithms, and real-time RSS feed integration for automatic updates.",
        impact: [
          "95% accuracy in document retrieval",
          "Reduced research time from 4 hours to 5 minutes",
          "Processing 80,000+ financial documents",
          "1,000+ daily queries handled",
          "Featured in Business Today & MoneyControl",
        ],
        features: [
          "Natural language query interface",
          "Semantic search with vector embeddings",
          "Real-time RSS feed monitoring for new circulars",
          "Custom relevance ranking algorithm",
          "Source citation with exact page numbers",
          "Multi-document summarization",
          "Query history and analytics dashboard",
        ],
        techStack: {
          frontend: ["React", "TailwindCSS", "Chart.js"],
          backend: ["Python", "FastAPI", "LangChain"],
          ai: ["Vertex AI", "Gemini Pro", "ChromaDB", "Embeddings API"],
          cloud: ["Google Cloud Run", "BigQuery", "Pub/Sub", "Cloud Storage"],
        },
        screenshots: [
          {
            title: "Query Interface",
            url: "/images/projects/camslens-query.png",
          },
          {
            title: "Search Results",
            url: "/images/projects/camslens-results.png",
          },
          {
            title: "Analytics Dashboard",
            url: "/images/projects/camslens-dashboard.png",
          },
        ],
        codeSnippets: [
          {
            filename: "rag_pipeline.py",
            language: "python",
            code: `from langchain import VertexAI
from chromadb import ChromaDB

class RAGPipeline:
    def __init__(self):
        self.llm = VertexAI(model="gemini-pro")
        self.vectordb = ChromaDB(collection="financial_docs")
    
    async def query(self, question: str) -> dict:
        # Retrieve relevant document chunks
        docs = self.vectordb.similarity_search(
            query=question,
            k=5,
            filter={"doc_type": "circular"}
        )
        
        # Generate answer with citations
        response = await self.llm.agenerate(
            prompt=self._build_prompt(docs, question),
            temperature=0.1
        )
        
        return {
            "answer": response.text,
            "sources": [d.metadata for d in docs]
        }`,
          },
        ],
        learnings: [
          "Optimizing vector search for large document corpora (80k+ docs)",
          "Balancing accuracy vs latency in production RAG systems",
          "Building reliable RSS feed processors with error handling",
          "Implementing custom ranking beyond cosine similarity",
        ],
        timeline: "June 2024 - Present",
        role: "Lead Developer",
        team: "Solo + 2 Reviewers",
      },
    },

    // ==========================================
    // 2. NEXGENGAMING - E-COMMERCE PLATFORM
    // ==========================================
    {
      id: "2",
      name: "ðŸŽ® NexGenGaming - E-Commerce Platform",
      url: "https://gaming-ecommerce-fronted.vercel.app/",
      description:
        "Full-stack gaming e-commerce with real-time inventory, order tracking, and payment integration.",
      languages: [
        { name: "React", iconifyClass: "logos:react" },
        { name: "Node.js", iconifyClass: "logos:nodejs-icon" },
        { name: "MongoDB", iconifyClass: "logos:mongodb-icon" },
        { name: "Express", iconifyClass: "simple-icons:express" },
        { name: "Firebase", iconifyClass: "logos:firebase" },
        { name: "TailwindCSS", iconifyClass: "logos:tailwindcss-icon" },
        { name: "Vercel", iconifyClass: "logos:vercel-icon" },
      ],
      details: {
        problem:
          "Gaming enthusiasts in India struggle to find authentic gaming consoles and accessories. Existing platforms lack real-time inventory updates, have poor user experience, and don't provide transparent order tracking.",
        solution:
          "Built a modern, responsive e-commerce platform specifically for gaming products. Features real-time inventory sync, seamless checkout with multiple payment options, order tracking, and an admin dashboard for inventory management.",
        impact: [
          "100+ products listed with real-time stock updates",
          "Seamless checkout experience",
          "Mobile-first responsive design",
          "Admin dashboard for complete control",
        ],
        features: [
          "Product catalog with filtering & search",
          "Real-time inventory management",
          "Shopping cart with persistent storage",
          "Secure payment gateway integration",
          "Order tracking & history",
          "Admin dashboard for CRUD operations",
          "Responsive design for all devices",
          "User authentication with Firebase",
        ],
        techStack: {
          frontend: ["React", "TailwindCSS", "React Router", "Context API"],
          backend: ["Node.js", "Express", "MongoDB", "Mongoose"],
          cloud: ["Firebase Auth", "Vercel", "MongoDB Atlas"],
          tools: ["Stripe/Razorpay", "Cloudinary", "JWT"],
        },
        screenshots: [
          { title: "Home Page", url: "/images/projects/gaming-home.png" },
          {
            title: "Product Details",
            url: "/images/projects/gaming-product.png",
          },
          { title: "Shopping Cart", url: "/images/projects/gaming-cart.png" },
        ],
        codeSnippets: [
          {
            filename: "cartContext.js",
            language: "javascript",
            code: `import { createContext, useReducer } from 'react';

const CartContext = createContext();

const cartReducer = (state, action) => {
  switch (action.type) {
    case 'ADD_ITEM':
      const existing = state.find(i => i.id === action.payload.id);
      if (existing) {
        return state.map(i => 
          i.id === action.payload.id 
            ? { ...i, quantity: i.quantity + 1 }
            : i
        );
      }
      return [...state, { ...action.payload, quantity: 1 }];
    
    case 'REMOVE_ITEM':
      return state.filter(i => i.id !== action.payload);
    
    default:
      return state;
  }
};

export const CartProvider = ({ children }) => {
  const [cart, dispatch] = useReducer(cartReducer, []);
  return (
    <CartContext.Provider value={{ cart, dispatch }}>
      {children}
    </CartContext.Provider>
  );
};`,
          },
        ],
        learnings: [
          "Building scalable e-commerce architecture",
          "Real-time inventory sync across multiple sessions",
          "Payment gateway integration best practices",
          "Optimizing MongoDB queries for product search",
        ],
        timeline: "March 2024 - May 2024",
        role: "Full Stack Developer",
        team: "Solo Project",
      },
    },

    // ==========================================
    // 3. ROOMFLOW - AI BOOKING SYSTEM
    // ==========================================
    {
      id: "3",
      name: "ðŸ¢ RoomFlow - AI Booking System",
      url: "https://workspace-booking-system.vercel.app/",
      description:
        "AI-assisted workspace booking platform with dynamic pricing, real-time conflict prevention, and admin analytics.",
      languages: [
        { name: "React", iconifyClass: "logos:react" },
        { name: "TypeScript", iconifyClass: "logos:typescript-icon" },
        { name: "Node.js", iconifyClass: "logos:nodejs-icon" },
        { name: "Express", iconifyClass: "simple-icons:express" },
        { name: "Vite", iconifyClass: "logos:vitejs" },
        { name: "PostgreSQL", iconifyClass: "logos:postgresql" },
        { name: "TailwindCSS", iconifyClass: "logos:tailwindcss-icon" },
      ],
      details: {
        problem:
          "Managing workspace bookings manually leads to double-bookings, pricing inconsistencies, and no visibility into space utilization. Employees waste time coordinating meeting rooms and admins lack analytics.",
        solution:
          "Built an intelligent booking platform with AI-powered natural language booking ('Book me a room for 10 people tomorrow at 3 PM'), dynamic pricing based on demand, real-time conflict detection, and comprehensive analytics for admins.",
        impact: [
          "Zero double-booking incidents",
          "30% increase in space utilization",
          "Natural language booking support",
          "Real-time availability updates",
        ],
        features: [
          "Natural language booking with LLM",
          "Dynamic pricing based on peak hours",
          "Real-time conflict prevention",
          "Interactive calendar view",
          "Cancellation policies & refunds",
          "Admin analytics dashboard",
          "Email notifications",
          "Multi-timezone support",
        ],
        techStack: {
          frontend: [
            "React",
            "TypeScript",
            "Vite",
            "TailwindCSS",
            "FullCalendar",
          ],
          backend: ["Node.js", "Express", "TypeScript", "Prisma"],
          database: ["PostgreSQL", "Redis (caching)"],
          cloud: ["Vercel", "Render", "Supabase"],
        },
        screenshots: [
          {
            title: "Booking Calendar",
            url: "/images/projects/roomflow-calendar.png",
          },
          { title: "AI Booking Chat", url: "/images/projects/roomflow-ai.png" },
          {
            title: "Admin Dashboard",
            url: "/images/projects/roomflow-admin.png",
          },
        ],
        codeSnippets: [
          {
            filename: "dynamicPricing.ts",
            language: "typescript",
            code: `interface PricingConfig {
  basePrice: number;
  peakMultiplier: number;
  weekendMultiplier: number;
}

export function calculatePrice(
  slot: TimeSlot,
  config: PricingConfig
): number {
  let price = config.basePrice;
  
  // Peak hours: 9-11 AM, 2-4 PM
  const hour = new Date(slot.startTime).getHours();
  const isPeakHour = (hour >= 9 && hour <= 11) || 
                     (hour >= 14 && hour <= 16);
  
  if (isPeakHour) {
    price *= config.peakMultiplier; // 1.5x
  }
  
  // Weekend pricing
  const dayOfWeek = new Date(slot.startTime).getDay();
  if (dayOfWeek === 0 || dayOfWeek === 6) {
    price *= config.weekendMultiplier; // 0.8x discount
  }
  
  return Math.round(price * 100) / 100;
}`,
          },
        ],
        learnings: [
          "Implementing real-time conflict detection at scale",
          "Building dynamic pricing algorithms",
          "TypeScript best practices for full-stack apps",
          "LLM integration for natural language interfaces",
        ],
        timeline: "April 2024 - June 2024",
        role: "Full Stack Developer",
        team: "Solo Project",
      },
    },

    // ==========================================
    // 4. SEBI DEBARRED PAN EXTRACTION
    // ==========================================
    {
      id: "4",
      name: "ðŸ¦ SEBI Debarred PAN Extraction",
      url: "https://github.com/Laxman824/",
      description:
        "Cloud-native backend monitoring SEBI regulatory orders, extracting debarred PAN details using AI-powered document parsing.",
      languages: [
        { name: "Python", iconifyClass: "logos:python" },
        { name: "Google Cloud", iconifyClass: "logos:google-cloud" },
        { name: "Vertex AI", iconifyClass: "simple-icons:googlegemini" },
        { name: "Firestore", iconifyClass: "logos:firebase" },
        { name: "Pub/Sub", iconifyClass: "logos:google-cloud" },
        { name: "Docker", iconifyClass: "logos:docker-icon" },
      ],
      details: {
        problem:
          "SEBI regularly publishes orders debarring individuals from securities markets. Compliance teams manually read 100+ page PDF orders to extract PAN numbers of debarred entities - a time-consuming, error-prone process critical for KYC compliance.",
        solution:
          "Built an automated pipeline that monitors SEBI website for new orders, downloads PDFs, uses AI (Gemini) to extract PAN numbers and debarment details, and stores structured data in Firestore. Event-driven architecture ensures real-time processing.",
        impact: [
          "100% automation of PAN extraction",
          "Processing orders within minutes of publication",
          "Zero manual intervention required",
          "Structured compliance database",
        ],
        features: [
          "Automated SEBI website monitoring",
          "PDF download and text extraction",
          "AI-powered PAN number detection",
          "Debarment period extraction",
          "Duplicate detection and deduplication",
          "Real-time Pub/Sub event processing",
          "Firestore database with search",
          "Alert notifications for new orders",
        ],
        techStack: {
          backend: ["Python", "FastAPI", "BeautifulSoup", "PyPDF2"],
          ai: ["Vertex AI", "Gemini Pro", "Document AI"],
          cloud: ["Cloud Run", "Pub/Sub", "Cloud Scheduler", "Firestore"],
          tools: ["Docker", "Cloud Build", "Secret Manager"],
        },
        screenshots: [
          {
            title: "System Architecture",
            url: "/images/projects/sebi-arch.png",
          },
          { title: "Extracted Data", url: "/images/projects/sebi-data.png" },
        ],
        codeSnippets: [
          {
            filename: "pan_extractor.py",
            language: "python",
            code: `import re
from vertexai.generative_models import GenerativeModel

class PANExtractor:
    def __init__(self):
        self.model = GenerativeModel("gemini-pro")
        self.pan_pattern = r'[A-Z]{5}[0-9]{4}[A-Z]{1}'
    
    async def extract_pans(self, pdf_text: str) -> list[dict]:
        # First pass: Regex extraction
        regex_pans = re.findall(self.pan_pattern, pdf_text)
        
        # Second pass: AI extraction for context
        prompt = f"""
        Extract all PAN numbers from this SEBI order.
        For each PAN, provide:
        - PAN number
        - Name of person/entity
        - Debarment period
        
        Text: {pdf_text[:10000]}
        
        Return as JSON array.
        """
        
        response = await self.model.generate_content_async(prompt)
        ai_results = self._parse_response(response.text)
        
        # Merge and deduplicate
        return self._merge_results(regex_pans, ai_results)`,
          },
        ],
        learnings: [
          "Building event-driven architectures on GCP",
          "Combining regex and AI for robust extraction",
          "Handling unstructured PDF documents at scale",
          "Designing for regulatory compliance requirements",
        ],
        timeline: "August 2024 - October 2024",
        role: "Backend Developer",
        team: "Solo + Compliance Team Review",
      },
    },

    // ==========================================
    // 5. AI-POWERED EMAIL AUTOMATION
    // ==========================================
    {
      id: "5",
      name: "ðŸ“§ AI-Powered Email Automation",
      url: "https://github.com/Laxman824/",
      description:
        "Intelligent microservice classifying financial emails with 95% accuracy. Extracts PAN, ARN codes using LLM zero-shot learning.",
      languages: [
        { name: "Python", iconifyClass: "logos:python" },
        { name: "FastAPI", iconifyClass: "simple-icons:fastapi" },
        { name: "LLM", iconifyClass: "simple-icons:openai" },
        { name: "vLLM", iconifyClass: "simple-icons:openai" },
        { name: "Docker", iconifyClass: "logos:docker-icon" },
        { name: "REST API", iconifyClass: "mdi:api" },
      ],
      details: {
        problem:
          "CAMS receives 1,000+ emails daily from mutual fund distributors, investors, and AMCs. Each email needs classification (complaint, query, transaction request) and extraction of key identifiers (PAN, Folio, ARN) for routing. Manual processing was slow and inconsistent.",
        solution:
          "Built an AI microservice using LLMs for zero-shot email classification and named entity extraction. The system classifies emails into 15+ categories, extracts structured data (PAN, dates, amounts), and routes to appropriate teams - all without training data.",
        impact: [
          "95% classification accuracy",
          "Processing 1,000+ emails daily",
          "80% reduction in manual work",
          "Sub-second response time",
        ],
        features: [
          "Zero-shot email classification (15+ categories)",
          "PAN, ARN, Folio number extraction",
          "Date and amount detection",
          "Sentiment analysis for prioritization",
          "Confidence scoring for human review",
          "REST API for integration",
          "Batch processing support",
          "Audit logging for compliance",
        ],
        techStack: {
          backend: ["Python", "FastAPI", "Pydantic", "asyncio"],
          ai: ["vLLM", "Mistral-7B", "LangChain", "Instructor"],
          infrastructure: ["Docker", "Kubernetes", "Redis"],
          monitoring: ["Prometheus", "Grafana", "Sentry"],
        },
        screenshots: [
          { title: "API Documentation", url: "/images/projects/email-api.png" },
          {
            title: "Classification Results",
            url: "/images/projects/email-results.png",
          },
        ],
        codeSnippets: [
          {
            filename: "email_classifier.py",
            language: "python",
            code: `from pydantic import BaseModel
from instructor import patch
from vllm import LLM

class EmailClassification(BaseModel):
    category: str
    confidence: float
    extracted_pan: str | None
    extracted_arn: str | None
    sentiment: str
    priority: str

class EmailClassifier:
    def __init__(self):
        self.llm = patch(LLM(model="mistralai/Mistral-7B"))
    
    async def classify(self, email: str) -> EmailClassification:
        response = await self.llm.chat.completions.create(
            messages=[{
                "role": "system",
                "content": "Classify financial service emails."
            }, {
                "role": "user", 
                "content": email
            }],
            response_model=EmailClassification
        )
        return response`,
          },
        ],
        learnings: [
          "Deploying LLMs in production with vLLM",
          "Structured output generation with Instructor",
          "Building high-throughput async APIs",
          "Zero-shot vs fine-tuned model tradeoffs",
        ],
        timeline: "July 2024 - September 2024",
        role: "ML Engineer",
        team: "Solo + Integration Team",
      },
    },

    // ==========================================
    // 6. IPO MONITORING SYSTEM
    // ==========================================
    {
      id: "6",
      name: "ðŸ“ˆ IPO Monitoring System",
      url: "https://ipo-monitor-gmp.streamlit.app/Dashboard",
      description:
        "Real-time IPO Grey Market Premium monitoring with automated notifications for investment decisions.",
      languages: [
        { name: "Python", iconifyClass: "logos:python" },
        { name: "Streamlit", iconifyClass: "simple-icons:streamlit" },
        { name: "Pandas", iconifyClass: "simple-icons:pandas" },
        { name: "Web Scraping", iconifyClass: "mdi:spider-web" },
      ],
      details: {
        problem:
          "IPO investors need to track Grey Market Premium (GMP) across multiple sources to make informed listing day decisions. Manually checking websites multiple times daily is tedious, and investors often miss optimal buying/selling windows.",
        solution:
          "Built a real-time dashboard that scrapes GMP data from multiple sources, tracks historical trends, calculates expected listing gains, and sends automated alerts when GMP crosses user-defined thresholds.",
        impact: [
          "Real-time GMP tracking for 20+ IPOs",
          "Historical trend analysis",
          "Automated price alerts",
          "500+ active users",
        ],
        features: [
          "Live GMP data from multiple sources",
          "Historical GMP trend charts",
          "Expected listing price calculator",
          "Subscription status tracking",
          "Category-wise analysis (Retail, HNI, QIB)",
          "Price alert notifications",
          "IPO calendar with key dates",
          "Comparison across IPOs",
        ],
        techStack: {
          frontend: ["Streamlit", "Plotly", "Altair"],
          backend: ["Python", "Pandas", "BeautifulSoup", "Requests"],
          database: ["SQLite", "CSV caching"],
          deployment: ["Streamlit Cloud", "GitHub Actions"],
        },
        screenshots: [
          {
            title: "Dashboard Overview",
            url: "/images/projects/ipo-dashboard.png",
          },
          { title: "GMP Trends", url: "/images/projects/ipo-trends.png" },
        ],
        codeSnippets: [
          {
            filename: "gmp_scraper.py",
            language: "python",
            code: `import pandas as pd
from bs4 import BeautifulSoup
import requests

class GMPScraper:
    def __init__(self):
        self.sources = [
            "https://ipowatch.in/ipo-grey-market-premium/",
            "https://investorgain.com/ipo-gmp/"
        ]
    
    def fetch_gmp_data(self) -> pd.DataFrame:
        all_data = []
        
        for url in self.sources:
            soup = BeautifulSoup(
                requests.get(url).content, 
                'html.parser'
            )
            table = soup.find('table', class_='gmp-table')
            
            for row in table.find_all('tr')[1:]:
                cols = row.find_all('td')
                all_data.append({
                    'ipo_name': cols[0].text.strip(),
                    'gmp': self._parse_gmp(cols[1].text),
                    'expected_listing': cols[2].text,
                    'source': url
                })
        
        return pd.DataFrame(all_data).drop_duplicates()`,
          },
        ],
        learnings: [
          "Building real-time data scrapers with rate limiting",
          "Creating interactive dashboards with Streamlit",
          "Handling inconsistent data from multiple sources",
          "Deploying data apps on Streamlit Cloud",
        ],
        timeline: "January 2024 - February 2024",
        role: "Full Stack Developer",
        team: "Solo Project",
      },
    },

    // ==========================================
    // 7. AUTONOMOUS MULTI-AGENT BANKING
    // ==========================================
    {
      id: "7",
      name: "ðŸ¤– Autonomous Multi-Agent Banking",
      url: "https://github.com/laxman824",
      description:
        "Advanced agentic workflow with 7 specialized AI agents using CrewAI and Gemini 2.5 for banking operations.",
      languages: [
        { name: "CrewAI", iconifyClass: "fluent:bot-sparkle-24-regular" },
        { name: "LangChain", iconifyClass: "simple-icons:langchain" },
        { name: "Vertex AI", iconifyClass: "simple-icons:googlegemini" },
        { name: "Python", iconifyClass: "logos:python" },
        { name: "Google Cloud", iconifyClass: "logos:google-cloud" },
      ],
      details: {
        problem:
          "Banking operations involve complex multi-step workflows (loan processing, KYC verification, fraud detection) that require coordination between multiple departments. Manual handoffs cause delays, errors, and poor customer experience.",
        solution:
          "Built an autonomous multi-agent system where specialized AI agents (Receptionist, Validator, Loan Specialist, Fraud Detector, etc.) collaborate to handle complex banking requests. Agents autonomously route tasks, validate inputs, and escalate when needed.",
        impact: [
          "7 specialized AI agents working in coordination",
          "Autonomous task routing and delegation",
          "90% reduction in processing time",
          "24/7 availability for customer requests",
        ],
        features: [
          "Receptionist Agent for intent detection",
          "Validator Agent for input verification",
          "Loan Specialist for credit assessment",
          "KYC Agent for identity verification",
          "Fraud Detection Agent for anomaly detection",
          "Escalation Agent for human handoff",
          "Audit Agent for compliance logging",
          "Inter-agent communication protocol",
        ],
        techStack: {
          framework: ["CrewAI", "LangChain", "LangGraph"],
          ai: ["Gemini 2.5 Pro", "Vertex AI", "Function Calling"],
          backend: ["Python", "FastAPI", "Redis"],
          cloud: ["Google Cloud Run", "Pub/Sub", "Firestore"],
        },
        screenshots: [
          {
            title: "Agent Architecture",
            url: "/images/projects/agent-arch.png",
          },
          {
            title: "Workflow Visualization",
            url: "/images/projects/agent-flow.png",
          },
        ],
        codeSnippets: [
          {
            filename: "banking_crew.py",
            language: "python",
            code: `from crewai import Agent, Task, Crew
from langchain_google_vertexai import ChatVertexAI

# Define specialized agents
receptionist = Agent(
    role="Customer Receptionist",
    goal="Understand customer intent and route appropriately",
    backstory="Expert at understanding banking queries",
    llm=ChatVertexAI(model="gemini-2.5-pro")
)

loan_specialist = Agent(
    role="Loan Specialist", 
    goal="Assess loan eligibility and terms",
    backstory="20 years of lending experience",
    llm=ChatVertexAI(model="gemini-2.5-pro")
)

fraud_detector = Agent(
    role="Fraud Analyst",
    goal="Detect suspicious patterns in transactions",
    backstory="Former forensic accountant",
    llm=ChatVertexAI(model="gemini-2.5-pro")
)

# Create crew with hierarchical process
banking_crew = Crew(
    agents=[receptionist, loan_specialist, fraud_detector],
    process="hierarchical",
    manager_llm=ChatVertexAI(model="gemini-2.5-pro")
)`,
          },
        ],
        learnings: [
          "Designing multi-agent communication protocols",
          "Handling agent failures and fallbacks gracefully",
          "Balancing autonomy with human oversight",
          "Optimizing token usage across multiple agents",
        ],
        timeline: "October 2024 - December 2024",
        role: "AI Engineer",
        team: "Solo + Banking SME Consultation",
      },
    },

    // ==========================================
    // 8. RSS DOCUMENT DOWNLOADER
    // ==========================================
    {
      id: "8",
      name: "ðŸ“„ RSS Document Downloader",
      url: "https://rss-feed-reader-and-pdf-downloader.streamlit.app/",
      description:
        "Automated document downloading from RSS feeds for real-time regulatory content acquisition.",
      languages: [
        { name: "Python", iconifyClass: "logos:python" },
        { name: "RSS", iconifyClass: "mdi:rss" },
        {
          name: "PDF Processing",
          iconifyClass: "simple-icons:adobeacrobatreader",
        },
        { name: "Streamlit", iconifyClass: "simple-icons:streamlit" },
      ],
      details: {
        problem:
          "Compliance teams need to monitor multiple regulatory RSS feeds (SEBI, RBI, AMFI) daily and download new circulars. Manual monitoring is tedious and risks missing critical updates.",
        solution:
          "Built an automated tool that monitors multiple RSS feeds, detects new documents, downloads PDFs automatically, organizes them by source/date, and provides a searchable interface for the document library.",
        impact: [
          "Monitoring 10+ regulatory RSS feeds",
          "Automatic download of new documents",
          "Zero missed circulars",
          "Searchable document archive",
        ],
        features: [
          "Multi-feed RSS monitoring",
          "Automatic PDF download",
          "Duplicate detection",
          "Date-based organization",
          "Full-text search in documents",
          "Download history tracking",
          "Email notifications for new docs",
          "Scheduled monitoring jobs",
        ],
        techStack: {
          frontend: ["Streamlit"],
          backend: ["Python", "feedparser", "requests", "PyPDF2"],
          storage: ["Local filesystem", "SQLite"],
          deployment: ["Streamlit Cloud"],
        },
        screenshots: [
          {
            title: "Feed Dashboard",
            url: "/images/projects/rss-dashboard.png",
          },
          {
            title: "Document Library",
            url: "/images/projects/rss-library.png",
          },
        ],
        codeSnippets: [
          {
            filename: "feed_monitor.py",
            language: "python",
            code: `import feedparser
import requests
from pathlib import Path

class FeedMonitor:
    def __init__(self, feeds: list[str]):
        self.feeds = feeds
        self.download_dir = Path("downloads")
        self.download_dir.mkdir(exist_ok=True)
    
    def check_new_documents(self) -> list[dict]:
        new_docs = []
        
        for feed_url in self.feeds:
            feed = feedparser.parse(feed_url)
            
            for entry in feed.entries:
                if self._is_new(entry):
                    pdf_url = self._extract_pdf_link(entry)
                    if pdf_url:
                        filepath = self._download(pdf_url, entry.title)
                        new_docs.append({
                            "title": entry.title,
                            "date": entry.published,
                            "path": filepath
                        })
        
        return new_docs`,
          },
        ],
        learnings: [
          "Parsing various RSS feed formats reliably",
          "Handling network failures with retries",
          "Building incremental sync mechanisms",
          "Creating user-friendly document management UIs",
        ],
        timeline: "February 2024",
        role: "Developer",
        team: "Solo Project",
      },
    },

    // ==========================================
    // 9. MSG TO PDF CONVERTER
    // ==========================================
    {
      id: "9",
      name: "ðŸ“¨ MSG to PDF Converter",
      url: "https://msg-to-pdf-converter-cams.streamlit.app/",
      description:
        "Batch converter for Outlook .MSG files to PDF. Processed 20,000+ financial emails with attachments.",
      languages: [
        { name: "Python", iconifyClass: "logos:python" },
        { name: "Outlook", iconifyClass: "logos:microsoft-outlook" },
        { name: "WeasyPrint", iconifyClass: "mdi:printer" },
        { name: "Streamlit", iconifyClass: "simple-icons:streamlit" },
      ],
      details: {
        problem:
          "Compliance requires archiving emails as PDFs for audit trails. Outlook .MSG files aren't easily searchable or viewable without Outlook. Converting 20,000+ emails manually was impossible.",
        solution:
          "Built a batch conversion tool that extracts MSG file contents (headers, body, attachments), renders them as formatted HTML, and converts to searchable PDFs. Handles complex scenarios like embedded images and nested attachments.",
        impact: [
          "Processed 20,000+ emails",
          "Preserved all formatting and attachments",
          "95% faster than manual conversion",
          "Searchable PDF output",
        ],
        features: [
          "Batch .MSG file processing",
          "HTML body rendering",
          "Attachment extraction and embedding",
          "Email header preservation",
          "Inline image handling",
          "Progress tracking for large batches",
          "Error handling and logging",
          "Zip download for batch results",
        ],
        techStack: {
          frontend: ["Streamlit"],
          backend: ["Python", "extract-msg", "WeasyPrint"],
          processing: ["BeautifulSoup", "Pillow"],
          deployment: ["Streamlit Cloud"],
        },
        screenshots: [
          { title: "Upload Interface", url: "/images/projects/msg-upload.png" },
          { title: "Converted PDF", url: "/images/projects/msg-result.png" },
        ],
        codeSnippets: [
          {
            filename: "msg_converter.py",
            language: "python",
            code: `import extract_msg
from weasyprint import HTML
from pathlib import Path

class MSGtoPDFConverter:
    def convert(self, msg_path: Path) -> Path:
        # Extract MSG contents
        msg = extract_msg.Message(msg_path)
        
        # Build HTML email template
        html = f"""
        <html>
        <head><style>
            body {{ font-family: Arial; }}
            .header {{ background: #f0f0f0; padding: 10px; }}
            .body {{ padding: 20px; }}
        </style></head>
        <body>
            <div class="header">
                <p><b>From:</b> {msg.sender}</p>
                <p><b>To:</b> {msg.to}</p>
                <p><b>Subject:</b> {msg.subject}</p>
                <p><b>Date:</b> {msg.date}</p>
            </div>
            <div class="body">{msg.htmlBody or msg.body}</div>
        </body>
        </html>
        """
        
        # Convert to PDF
        pdf_path = msg_path.with_suffix('.pdf')
        HTML(string=html).write_pdf(pdf_path)
        
        return pdf_path`,
          },
        ],
        learnings: [
          "Parsing Microsoft MSG file format",
          "Rendering HTML to PDF with WeasyPrint",
          "Handling various email encodings",
          "Building robust batch processing pipelines",
        ],
        timeline: "March 2024",
        role: "Developer",
        team: "Solo Project",
      },
    },

    // ==========================================
    // 10. PDF FOOTNOTES PROCESSOR
    // ==========================================
    {
      id: "10",
      name: "ðŸ“ PDF Footnotes Processor",
      url: "https://pdf-proceappr-app-jxybczrguea2egjbmrk6yh.streamlit.app/",
      description:
        "Automated footnotes and reference mapping for academic documents with Excel export.",
      languages: [
        { name: "OpenCV", iconifyClass: "logos:opencv" },
        { name: "OCR", iconifyClass: "mdi:ocr" },
        {
          name: "PDF Processing",
          iconifyClass: "simple-icons:adobeacrobatreader",
        },
        { name: "Streamlit", iconifyClass: "simple-icons:streamlit" },
      ],
      details: {
        problem:
          "Academic researchers spend hours manually extracting and organizing footnotes from PDF documents. Matching footnote numbers to their references across pages is tedious and error-prone.",
        solution:
          "Built a tool that uses OCR and layout analysis to detect footnote regions, extract text, match footnote numbers to references, and export as a structured Excel spreadsheet with page numbers.",
        impact: [
          "90% time savings on footnote extraction",
          "Accurate footnote-reference matching",
          "Structured Excel output",
          "Works with scanned PDFs",
        ],
        features: [
          "PDF text extraction with layout preservation",
          "Footnote region detection using CV",
          "Footnote number-reference matching",
          "Multi-page reference tracking",
          "Excel export with formatting",
          "Scanned PDF support via OCR",
          "Batch processing capability",
          "Preview before export",
        ],
        techStack: {
          frontend: ["Streamlit"],
          backend: ["Python", "PyMuPDF", "pdfplumber"],
          cv: ["OpenCV", "Tesseract OCR"],
          export: ["openpyxl", "pandas"],
        },
        screenshots: [
          {
            title: "Processing View",
            url: "/images/projects/footnotes-process.png",
          },
          {
            title: "Excel Output",
            url: "/images/projects/footnotes-excel.png",
          },
        ],
        codeSnippets: [
          {
            filename: "footnote_extractor.py",
            language: "python",
            code: `import pdfplumber
import re

class FootnoteExtractor:
    def __init__(self):
        self.footnote_pattern = r'^\\d+\\.?\\s+'
    
    def extract(self, pdf_path: str) -> list[dict]:
        footnotes = []
        
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                # Get bottom 20% of page (typical footnote region)
                height = page.height
                footnote_region = page.crop((0, height*0.8, page.width, height))
                text = footnote_region.extract_text()
                
                if text:
                    for line in text.split('\\n'):
                        match = re.match(self.footnote_pattern, line)
                        if match:
                            footnotes.append({
                                'number': match.group().strip(),
                                'text': line[match.end():],
                                'page': page_num + 1
                            })
        
        return footnotes`,
          },
        ],
        learnings: [
          "PDF layout analysis techniques",
          "Combining rule-based and ML approaches",
          "Handling diverse PDF formats",
          "Building academic research tools",
        ],
        timeline: "December 2023",
        role: "Developer",
        team: "Solo Project",
      },
    },

    // ==========================================
    // 11. OCR FIELD EXTRACTION
    // ==========================================
    {
      id: "11",
      name: "ðŸ” OCR Field Extraction",
      url: "https://ocr-field-extraction.streamlit.app/",
      description:
        "Automated extraction of PAN, Folio numbers from scanned documents using DocTR and custom post-processing.",
      languages: [
        { name: "OpenCV", iconifyClass: "logos:opencv" },
        { name: "DocTR", iconifyClass: "simple-icons:readme" },
        { name: "Python", iconifyClass: "logos:python" },
        { name: "Streamlit", iconifyClass: "simple-icons:streamlit" },
      ],
      details: {
        problem:
          "Financial services require extracting specific fields (PAN, Folio, Account numbers) from scanned documents. Traditional OCR gives raw text - identifying and extracting specific fields requires manual effort.",
        solution:
          "Built a specialized OCR pipeline using DocTR for text detection and custom regex/AI post-processing to identify and extract specific financial identifiers with high accuracy.",
        impact: [
          "98% accuracy on PAN extraction",
          "Processing 100+ documents/minute",
          "Handles rotated and skewed images",
          "JSON API for integration",
        ],
        features: [
          "Multi-format image support (JPG, PNG, TIFF, PDF)",
          "PAN number detection and validation",
          "Folio number extraction",
          "Account number identification",
          "Date extraction in multiple formats",
          "Confidence scoring",
          "Image preprocessing (deskew, denoise)",
          "REST API endpoint",
        ],
        techStack: {
          frontend: ["Streamlit"],
          backend: ["Python", "FastAPI"],
          ocr: ["DocTR", "Tesseract (fallback)"],
          cv: ["OpenCV", "Pillow", "scikit-image"],
        },
        screenshots: [
          { title: "Upload Interface", url: "/images/projects/ocr-upload.png" },
          {
            title: "Extraction Results",
            url: "/images/projects/ocr-results.png",
          },
        ],
        codeSnippets: [
          {
            filename: "field_extractor.py",
            language: "python",
            code: `from doctr.io import DocumentFile
from doctr.models import ocr_predictor
import re

class FieldExtractor:
    def __init__(self):
        self.model = ocr_predictor(pretrained=True)
        self.pan_pattern = r'[A-Z]{5}[0-9]{4}[A-Z]'
        self.folio_pattern = r'\\d{10,15}'
    
    def extract(self, image_path: str) -> dict:
        # Run OCR
        doc = DocumentFile.from_images(image_path)
        result = self.model(doc)
        
        # Get all text
        text = " ".join([
            word.value 
            for page in result.pages 
            for block in page.blocks 
            for line in block.lines 
            for word in line.words
        ])
        
        # Extract fields
        return {
            'pan': self._find_pan(text),
            'folio': self._find_folio(text),
            'raw_text': text
        }
    
    def _find_pan(self, text: str) -> str | None:
        match = re.search(self.pan_pattern, text.upper())
        return match.group() if match else None`,
          },
        ],
        learnings: [
          "DocTR vs Tesseract performance tradeoffs",
          "Image preprocessing for better OCR",
          "Validation rules for financial identifiers",
          "Building production OCR APIs",
        ],
        timeline: "November 2023",
        role: "Developer",
        team: "Solo Project",
      },
    },

    // ==========================================
    // 12. STOCK EXCHANGE PLATFORM
    // ==========================================
    {
      id: "12",
      name: "ðŸ’¹ Stock Exchange Platform",
      url: "https://github.com/Laxman824/CAMS-StockEx",
      description:
        "Full-stack stock trading platform with Spring Boot backend, real-time prices, and portfolio management.",
      languages: [
        { name: "Spring Boot", iconifyClass: "logos:spring-icon" },
        { name: "Java", iconifyClass: "logos:java" },
        { name: "REST API", iconifyClass: "mdi:api" },
        { name: "Database", iconifyClass: "mdi:database" },
      ],
      details: {
        problem:
          "Learning stock trading systems requires understanding order matching, portfolio management, and real-time data handling. Existing tutorials are superficial and don't cover production concerns.",
        solution:
          "Built a complete stock exchange simulation with order book management, limit/market orders, portfolio tracking, and simulated price feeds. Demonstrates enterprise Java patterns and real-time data handling.",
        impact: [
          "Complete trading simulation",
          "Order matching engine implementation",
          "Real-time portfolio valuation",
          "Educational resource for learners",
        ],
        features: [
          "User registration and authentication",
          "Stock listing and search",
          "Market and limit order placement",
          "Order book visualization",
          "Trade execution and matching",
          "Portfolio tracking with P&L",
          "Historical transaction log",
          "Price chart simulation",
        ],
        techStack: {
          frontend: ["React", "Chart.js", "Bootstrap"],
          backend: ["Spring Boot", "Spring Security", "JPA/Hibernate"],
          database: ["PostgreSQL", "H2 (testing)"],
          tools: ["Maven", "JUnit", "Swagger"],
        },
        screenshots: [
          {
            title: "Trading Dashboard",
            url: "/images/projects/stock-dashboard.png",
          },
          { title: "Order Book", url: "/images/projects/stock-orderbook.png" },
        ],
        codeSnippets: [
          {
            filename: "OrderMatchingService.java",
            language: "java",
            code: `@Service
public class OrderMatchingService {
    
    @Autowired
    private OrderRepository orderRepository;
    
    @Transactional
    public Trade matchOrder(Order incomingOrder) {
        List<Order> matchingOrders = orderRepository
            .findMatchingOrders(
                incomingOrder.getSymbol(),
                incomingOrder.getSide().opposite(),
                incomingOrder.getPrice()
            );
        
        for (Order existingOrder : matchingOrders) {
            if (canMatch(incomingOrder, existingOrder)) {
                Trade trade = executeTrade(
                    incomingOrder, 
                    existingOrder
                );
                return trade;
            }
        }
        
        // No match - add to order book
        orderRepository.save(incomingOrder);
        return null;
    }
}`,
          },
        ],
        learnings: [
          "Order matching engine algorithms",
          "Spring Boot enterprise patterns",
          "Database transaction management",
          "Real-time data with WebSockets",
        ],
        timeline: "September 2023 - November 2023",
        role: "Full Stack Developer",
        team: "Solo Project",
      },
    },

    // ==========================================
    // 13. DRIVER DROWSINESS DETECTION
    // ==========================================
    {
      id: "13",
      name: "ðŸ˜´ Driver Drowsiness Detection",
      url:
        "https://github.com/Laxman824/Projects-Assignments/tree/main/Realtime%20driver%20drowsiness%20detection",
      description:
        "Real-time embedded system for driver safety using Raspberry Pi with OpenCV and dlib.",
      languages: [
        { name: "OpenCV", iconifyClass: "logos:opencv" },
        { name: "dlib", iconifyClass: "simple-icons:cmake" },
        { name: "Embedded C", iconifyClass: "logos:c" },
        { name: "Raspberry Pi", iconifyClass: "logos:raspberry-pi" },
      ],
      details: {
        problem:
          "Drowsy driving causes 100,000+ accidents annually. Commercial solutions are expensive and not accessible. A low-cost, real-time detection system could save lives.",
        solution:
          "Built an embedded system using Raspberry Pi and camera that detects eye closure patterns (Eye Aspect Ratio) and yawning using facial landmark detection. Triggers audio alerts when drowsiness is detected.",
        impact: [
          "Real-time detection at 30 FPS",
          "95% accuracy in controlled conditions",
          "Low-cost hardware (~$50)",
          "Sub-second alert latency",
        ],
        features: [
          "Real-time face detection",
          "68-point facial landmark detection",
          "Eye Aspect Ratio (EAR) calculation",
          "Yawn detection via mouth aspect ratio",
          "Configurable alert thresholds",
          "Audio alarm system",
          "Low-light mode with IR camera",
          "Data logging for analysis",
        ],
        techStack: {
          hardware: ["Raspberry Pi 4", "Pi Camera", "Speaker"],
          software: ["Python", "OpenCV", "dlib", "NumPy"],
          algorithm: ["Facial Landmarks", "EAR Algorithm", "MAR Algorithm"],
        },
        screenshots: [
          { title: "System Setup", url: "/images/projects/drowsy-setup.png" },
          { title: "Detection Demo", url: "/images/projects/drowsy-demo.png" },
        ],
        codeSnippets: [
          {
            filename: "drowsiness_detector.py",
            language: "python",
            code: `import cv2
import dlib
from scipy.spatial import distance

class DrowsinessDetector:
    def __init__(self):
        self.detector = dlib.get_frontal_face_detector()
        self.predictor = dlib.shape_predictor(
            "shape_predictor_68_face_landmarks.dat"
        )
        self.EAR_THRESHOLD = 0.25
        self.CONSEC_FRAMES = 20
        self.counter = 0
    
    def eye_aspect_ratio(self, eye):
        # Vertical distances
        A = distance.euclidean(eye[1], eye[5])
        B = distance.euclidean(eye[2], eye[4])
        # Horizontal distance
        C = distance.euclidean(eye[0], eye[3])
        return (A + B) / (2.0 * C)
    
    def detect(self, frame):
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.detector(gray)
        
        for face in faces:
            landmarks = self.predictor(gray, face)
            left_eye = self._get_eye(landmarks, 36, 42)
            right_eye = self._get_eye(landmarks, 42, 48)
            
            ear = (self.eye_aspect_ratio(left_eye) + 
                   self.eye_aspect_ratio(right_eye)) / 2
            
            if ear < self.EAR_THRESHOLD:
                self.counter += 1
                if self.counter >= self.CONSEC_FRAMES:
                    return True  # DROWSY!
            else:
                self.counter = 0
        
        return False`,
          },
        ],
        learnings: [
          "Real-time computer vision on embedded systems",
          "Facial landmark detection with dlib",
          "Optimizing OpenCV for Raspberry Pi",
          "Building safety-critical systems",
        ],
        timeline: "August 2022 - November 2022",
        role: "Developer",
        team: "IIT Delhi Project Team (3 members)",
      },
    },

    // ==========================================
    // 14. RAVI - VISUAL ASSISTANT
    // ==========================================
    {
      id: "14",
      name: "ðŸ‘ï¸ RAVI - Visual Assistant",
      url: "https://github.com/Laxman824/RAVI-2022-2023-",
      description:
        "AI-powered Reading Assistant for Visually Impaired using computer vision and automated image descriptions.",
      languages: [
        { name: "Azure", iconifyClass: "logos:microsoft-azure" },
        { name: "JavaScript", iconifyClass: "logos:javascript" },
        { name: "Python", iconifyClass: "logos:python" },
        { name: "Docker", iconifyClass: "logos:docker-icon" },
      ],
      details: {
        problem:
          "Visually impaired students struggle to access STEM education because textbooks contain diagrams, charts, and equations that screen readers can't interpret. Manual description creation is slow and expensive.",
        solution:
          "Built RAVI (Reading Assistant for Visually Impaired) - an AI system that automatically generates accurate descriptions for images in STEM documents, with crowdsourced verification for quality assurance.",
        impact: [
          "Automated descriptions for STEM images",
          "Crowdsourced verification workflow",
          "Deployed at IIT Delhi Assistech Lab",
          "Impacting 100+ visually impaired students",
        ],
        features: [
          "Automatic image description generation",
          "STEM-specific vocabulary handling",
          "Mathematical equation description",
          "Crowdsourced verification portal",
          "Audio output integration",
          "PDF document processing",
          "ALT text generation",
          "Quality scoring system",
        ],
        techStack: {
          frontend: ["JavaScript", "HTML/CSS", "Bootstrap"],
          backend: ["Python", "Flask", "Azure Functions"],
          ai: ["Azure Computer Vision", "GPT-4", "Custom models"],
          cloud: ["Azure Blob Storage", "Azure SQL", "Docker"],
        },
        screenshots: [
          {
            title: "Description Interface",
            url: "/images/projects/ravi-interface.png",
          },
          {
            title: "Verification Portal",
            url: "/images/projects/ravi-verify.png",
          },
        ],
        codeSnippets: [
          {
            filename: "image_describer.py",
            language: "python",
            code: `from azure.cognitiveservices.vision.computervision import ComputerVisionClient
from openai import OpenAI

class STEMImageDescriber:
    def __init__(self):
        self.cv_client = ComputerVisionClient(
            endpoint=AZURE_ENDPOINT,
            credentials=AZURE_CREDENTIALS
        )
        self.llm = OpenAI()
    
    async def describe(self, image_url: str, context: str) -> str:
        # Get base description from Azure CV
        analysis = self.cv_client.analyze_image(
            image_url,
            visual_features=['Description', 'Objects', 'Tags']
        )
        
        # Enhance with GPT for STEM context
        enhanced = await self.llm.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=[{
                "role": "user",
                "content": f"""
                Describe this STEM image for a visually impaired student.
                Context: {context}
                Azure analysis: {analysis.description}
                
                Provide a detailed, educational description.
                """
            }]
        )
        
        return enhanced.choices[0].message.content`,
          },
        ],
        learnings: [
          "Building accessible technology solutions",
          "Azure Computer Vision APIs",
          "Crowdsourcing for quality assurance",
          "Working with visually impaired users",
        ],
        timeline: "August 2022 - May 2024",
        role: "Software Engineer",
        team: "RAVI Team at IIT Delhi Assistech Lab",
      },
    },

    // ==========================================
    // 15. TABLE RECOGNITION SYSTEM
    // ==========================================
    {
      id: "15",
      name: "ðŸ“Š Table Recognition System",
      url: "https://github.com/Laxman824/MTP2-work",
      description:
        "Deep learning system for borderless table detection and recognition in PDF documents.",
      languages: [
        {
          name: "Deep Learning",
          iconifyClass: "carbon:machine-learning-model",
        },
        { name: "Python", iconifyClass: "logos:python" },
        { name: "MTL-Tabnet", iconifyClass: "mdi:table" },
        { name: "PyTorch", iconifyClass: "logos:pytorch-icon" },
      ],
      details: {
        problem:
          "Tables in PDF documents, especially borderless tables, are difficult for screen readers to interpret. Existing table detection works only for bordered tables, leaving visually impaired users unable to access tabular data.",
        solution:
          "Developed a deep learning model (MTL-TabNet architecture) that detects table regions, identifies rows and columns even without visible borders, and extracts structured data for accessibility tools.",
        impact: [
          "85% accuracy on borderless tables",
          "Published research at IIT Delhi",
          "Integrated into accessibility pipeline",
          "Open-source contribution",
        ],
        features: [
          "Borderless table detection",
          "Row and column segmentation",
          "Cell content extraction",
          "Multi-page table handling",
          "Export to CSV/Excel",
          "Integration with screen readers",
          "Scanned document support",
          "Table structure reconstruction",
        ],
        techStack: {
          ml: ["PyTorch", "MTL-TabNet", "Transformers"],
          cv: ["OpenCV", "Detectron2"],
          data: ["pandas", "NumPy", "scikit-learn"],
          tools: ["Weights & Biases", "Jupyter", "LaTeX"],
        },
        screenshots: [
          {
            title: "Table Detection",
            url: "/images/projects/table-detect.png",
          },
          {
            title: "Structure Recognition",
            url: "/images/projects/table-structure.png",
          },
        ],
        codeSnippets: [
          {
            filename: "table_detector.py",
            language: "python",
            code: `import torch
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor

class TableDetector:
    def __init__(self, model_path: str):
        cfg = get_cfg()
        cfg.merge_from_file(model_zoo.get_config_file(
            "COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"
        ))
        cfg.MODEL.WEIGHTS = model_path
        cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # table, cell
        
        self.predictor = DefaultPredictor(cfg)
    
    def detect_tables(self, image):
        outputs = self.predictor(image)
        instances = outputs["instances"]
        
        tables = []
        for i, (box, score, cls) in enumerate(zip(
            instances.pred_boxes,
            instances.scores,
            instances.pred_classes
        )):
            if cls == 0 and score > 0.8:  # Table class
                tables.append({
                    'bbox': box.cpu().numpy(),
                    'confidence': score.item()
                })
        
        return tables`,
          },
        ],
        learnings: [
          "Training custom object detection models",
          "Document layout analysis techniques",
          "Academic research methodology",
          "Accessibility in AI applications",
        ],
        timeline: "January 2023 - May 2024 (M.Tech Thesis)",
        role: "Researcher",
        team: "Under Prof. Guidance at IIT Delhi",
      },
    },

    // ==========================================
    // 16. RAVI - ACCESSIBLE STEM EDUCATION
    // ==========================================
    {
      id: "16",
      name: "â™¿ RAVI - Accessible STEM Education",
      url: "https://github.com/Laxman824/RAVI-2022-2023-",
      description:
        "AI-powered accessibility tool for STEM documents enabling visually impaired students to access scientific content.",
      languages: [
        { name: "Python", iconifyClass: "logos:python" },
        { name: "Flask", iconifyClass: "simple-icons:flask" },
        { name: "HTML", iconifyClass: "logos:html-5" },
        {
          name: "Machine Learning",
          iconifyClass: "carbon:machine-learning-model",
        },
        { name: "Computer Vision", iconifyClass: "mdi:eye-outline" },
      ],
      details: {
        problem:
          "STEM education relies heavily on visual content - graphs, diagrams, chemical structures. Visually impaired students miss critical learning because these images lack proper descriptions.",
        solution:
          "Extended RAVI with STEM-specific capabilities: chemical structure recognition, graph interpretation, equation rendering, and context-aware descriptions that explain WHY an image is important, not just WHAT it shows.",
        impact: [
          "Supporting 100+ students at IIT Delhi",
          "Partnership with Assistech Lab",
          "Recognition from accessibility community",
          "Open-source toolkit release",
        ],
        features: [
          "Chemical structure recognition",
          "Graph and chart interpretation",
          "Mathematical equation audio rendering",
          "Context-aware descriptions",
          "Learning material organization",
          "Student progress tracking",
          "Teacher annotation tools",
          "Multi-language support",
        ],
        techStack: {
          frontend: ["HTML/CSS", "JavaScript", "Bootstrap"],
          backend: ["Python", "Flask", "SQLAlchemy"],
          ai: ["TensorFlow", "ChemBERTa", "GPT-4"],
          accessibility: ["ARIA", "Screen Reader APIs"],
        },
        screenshots: [
          { title: "STEM Dashboard", url: "/images/projects/ravi-stem.png" },
          {
            title: "Chemical Recognition",
            url: "/images/projects/ravi-chem.png",
          },
        ],
        codeSnippets: [
          {
            filename: "stem_describer.py",
            language: "python",
            code: `class STEMDescriber:
    def __init__(self):
        self.graph_analyzer = GraphAnalyzer()
        self.chem_recognizer = ChemicalRecognizer()
        self.equation_parser = EquationParser()
    
    def describe_image(self, image, image_type: str) -> str:
        if image_type == "graph":
            analysis = self.graph_analyzer.analyze(image)
            return f"""
            This is a {analysis.chart_type} showing {analysis.title}.
            The X-axis represents {analysis.x_label}.
            The Y-axis represents {analysis.y_label}.
            Key trend: {analysis.trend_description}.
            Maximum value: {analysis.max_value} at {analysis.max_point}.
            """
        
        elif image_type == "chemical":
            structure = self.chem_recognizer.recognize(image)
            return f"""
            Chemical structure: {structure.name}
            Formula: {structure.formula}
            This molecule has {structure.description}
            """
        
        # ... more handlers`,
          },
        ],
        learnings: [
          "Domain-specific AI applications",
          "Accessibility-first design thinking",
          "Collaboration with end-users",
          "Impact measurement in social projects",
        ],
        timeline: "August 2022 - May 2024",
        role: "Lead Developer",
        team: "RAVI Team (5 members)",
      },
    },

    // ==========================================
    // 17. SKYLINE AIRLINES - DBMS SYSTEM
    // ==========================================
    {
      id: "17",
      name: "âœˆï¸ SkyLine Airlines - DBMS System",
      url: "https://github.com/Laxman824/Dbms-Project",
      description:
        "Airline reservation system with Flask and PostgreSQL featuring advanced SQL queries and booking management.",
      languages: [
        { name: "Flask", iconifyClass: "simple-icons:flask" },
        { name: "PostgreSQL", iconifyClass: "logos:postgresql" },
        { name: "HTML/CSS", iconifyClass: "logos:html-5" },
        { name: "SQL", iconifyClass: "mdi:database" },
      ],
      details: {
        problem:
          "Database management course project requiring demonstration of advanced SQL concepts: complex joins, triggers, stored procedures, and transaction management in a real-world application.",
        solution:
          "Built a complete airline reservation system showcasing: flight search with multi-stop routing, seat inventory with overbooking, dynamic pricing, loyalty program, and comprehensive admin reporting.",
        impact: [
          "Comprehensive DBMS demonstration",
          "Complex SQL query optimization",
          "Transaction management implementation",
          "Top grade in DBMS course",
        ],
        features: [
          "User registration with hashed passwords",
          "Flight search with filters",
          "Multi-city routing algorithm",
          "Seat selection with class options",
          "Dynamic pricing based on demand",
          "Booking confirmation and e-tickets",
          "Loyalty points system",
          "Admin reports and analytics",
        ],
        techStack: {
          frontend: ["HTML", "CSS", "Jinja2", "Bootstrap"],
          backend: ["Python", "Flask", "SQLAlchemy"],
          database: ["PostgreSQL", "Stored Procedures", "Triggers"],
          tools: ["pgAdmin", "Postman"],
        },
        screenshots: [
          {
            title: "Flight Search",
            url: "/images/projects/airline-search.png",
          },
          {
            title: "Booking Flow",
            url: "/images/projects/airline-booking.png",
          },
        ],
        codeSnippets: [
          {
            filename: "booking_trigger.sql",
            language: "sql",
            code: `-- Trigger to update seat availability after booking
CREATE OR REPLACE FUNCTION update_seat_availability()
RETURNS TRIGGER AS $$
BEGIN
    -- Decrease available seats
    UPDATE flights 
    SET available_seats = available_seats - 1
    WHERE flight_id = NEW.flight_id;
    
    -- Check if flight is now full
    IF (SELECT available_seats FROM flights 
        WHERE flight_id = NEW.flight_id) = 0 THEN
        UPDATE flights 
        SET status = 'FULL'
        WHERE flight_id = NEW.flight_id;
    END IF;
    
    -- Award loyalty points
    INSERT INTO loyalty_transactions (user_id, points, type)
    VALUES (NEW.user_id, NEW.fare * 0.1, 'EARNED');
    
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER after_booking_insert
AFTER INSERT ON bookings
FOR EACH ROW
EXECUTE FUNCTION update_seat_availability();`,
          },
        ],
        learnings: [
          "Advanced SQL query optimization",
          "Database normalization principles",
          "Transaction ACID properties",
          "Building database triggers and procedures",
        ],
        timeline: "September 2021 - December 2021",
        role: "Developer",
        team: "DBMS Course Project (2 members)",
      },
    },

    // ==========================================
    // 18. BITTORRENT FILE TRANSFER
    // ==========================================
    {
      id: "18",
      name: "ðŸŒ BitTorrent File Transfer",
      url:
        "https://github.com/Laxman824/Projects-Assignments/tree/main/BitTorrent-technique",
      description:
        "Implementation of BitTorrent P2P protocol with parallel TCP connections for distributed file sharing.",
      languages: [
        { name: "Python", iconifyClass: "logos:python" },
        { name: "Networking", iconifyClass: "mdi:network" },
        { name: "TCP/IP", iconifyClass: "carbon:network-3" },
        { name: "P2P", iconifyClass: "mdi:share-variant" },
      ],
      details: {
        problem:
          "Understanding distributed systems requires hands-on implementation. BitTorrent's peer-to-peer architecture is an excellent case study for parallel downloads, chunk verification, and distributed coordination.",
        solution:
          "Implemented core BitTorrent concepts: torrent file parsing, tracker communication, peer discovery, parallel chunk downloading from multiple peers, SHA1 verification, and file reconstruction.",
        impact: [
          "Working P2P file transfer",
          "4x faster than sequential download",
          "Demonstrates distributed systems concepts",
          "Educational resource for networking",
        ],
        features: [
          "Torrent file (.torrent) parsing",
          "Tracker HTTP announce protocol",
          "Peer discovery and handshake",
          "Parallel downloads from multiple peers",
          "Piece selection algorithms (rarest first)",
          "SHA1 hash verification",
          "File reconstruction from chunks",
          "Progress tracking and resumption",
        ],
        techStack: {
          language: ["Python 3"],
          networking: ["socket", "asyncio", "aiohttp"],
          crypto: ["hashlib (SHA1)"],
          encoding: ["bencodepy"],
        },
        screenshots: [
          {
            title: "Download Progress",
            url: "/images/projects/torrent-progress.png",
          },
        ],
        codeSnippets: [
          {
            filename: "peer_connection.py",
            language: "python",
            code: `import asyncio
import struct

class PeerConnection:
    def __init__(self, ip: str, port: int, info_hash: bytes):
        self.ip = ip
        self.port = port
        self.info_hash = info_hash
        self.peer_id = self._generate_peer_id()
    
    async def connect(self):
        self.reader, self.writer = await asyncio.open_connection(
            self.ip, self.port
        )
        await self._handshake()
    
    async def _handshake(self):
        # BitTorrent handshake: <pstrlen><pstr><reserved><info_hash><peer_id>
        handshake = (
            b'\\x13BitTorrent protocol' +
            b'\\x00' * 8 +
            self.info_hash +
            self.peer_id
        )
        self.writer.write(handshake)
        await self.writer.drain()
        
        # Read peer handshake response
        response = await self.reader.read(68)
        if response[28:48] != self.info_hash:
            raise Exception("Info hash mismatch")
    
    async def request_piece(self, index: int, begin: int, length: int):
        # Request message: <len=13><id=6><index><begin><length>
        request = struct.pack('>IBIII', 13, 6, index, begin, length)
        self.writer.write(request)
        await self.writer.drain()`,
          },
        ],
        learnings: [
          "TCP socket programming in Python",
          "Binary protocol implementation",
          "Asynchronous I/O with asyncio",
          "Distributed systems coordination",
        ],
        timeline: "March 2021",
        role: "Developer",
        team: "Networks Course Assignment",
      },
    },

    // ==========================================
    // 19. AMUDALAKUNTA - PAYMENT GATEWAY
    // ==========================================
    {
      id: "19",
      name: "ðŸ’³ AmudalaKunta - Payment Gateway",
      url: "https://laxman824.github.io/AmudalaKunta/",
      description:
        "Live web application with integrated payment gateway and admin dashboard for village development fund.",
      languages: [
        { name: "HTML/CSS", iconifyClass: "logos:html-5" },
        { name: "JavaScript", iconifyClass: "logos:javascript" },
        { name: "Payment Gateway", iconifyClass: "mdi:credit-card-outline" },
        { name: "Dashboard", iconifyClass: "mdi:view-dashboard" },
      ],
      details: {
        problem:
          "My native village lacked a transparent system for collecting and tracking development fund contributions. Manual tracking was error-prone and lacked trust.",
        solution:
          "Built a website for the village with Razorpay payment integration, contribution tracking, donor recognition, and an admin dashboard for fund management and transparency.",
        impact: [
          "â‚¹2L+ contributions collected",
          "100% transparent tracking",
          "Digital payment adoption in village",
          "Community trust building",
        ],
        features: [
          "Village information and updates",
          "Secure payment gateway (Razorpay)",
          "Contribution tracking",
          "Donor wall with recognition tiers",
          "Admin dashboard for management",
          "Receipt generation",
          "Fund utilization reports",
          "Mobile-responsive design",
        ],
        techStack: {
          frontend: ["HTML5", "CSS3", "JavaScript", "Bootstrap"],
          payment: ["Razorpay"],
          hosting: ["GitHub Pages"],
          tools: ["Google Sheets (backend)", "EmailJS"],
        },
        screenshots: [
          { title: "Homepage", url: "/images/projects/village-home.png" },
          {
            title: "Payment Page",
            url: "/images/projects/village-payment.png",
          },
        ],
        codeSnippets: [
          {
            filename: "payment.js",
            language: "javascript",
            code: `const initPayment = async (amount, name, email) => {
  const options = {
    key: RAZORPAY_KEY,
    amount: amount * 100, // Razorpay uses paise
    currency: "INR",
    name: "AmudalaKunta Development Fund",
    description: "Village Development Contribution",
    image: "/logo.png",
    
    handler: async function(response) {
      // Payment successful
      await recordDonation({
        paymentId: response.razorpay_payment_id,
        name: name,
        amount: amount,
        date: new Date().toISOString()
      });
      
      showThankYou(name, amount);
      sendReceiptEmail(email, amount);
    },
    
    prefill: {
      name: name,
      email: email
    },
    
    theme: {
      color: "#1a73e8"
    }
  };
  
  const rzp = new Razorpay(options);
  rzp.open();
};`,
          },
        ],
        learnings: [
          "Payment gateway integration",
          "Building for real users",
          "Simple backends with Google Sheets",
          "Community-driven development",
        ],
        timeline: "December 2020",
        role: "Developer & Initiator",
        team: "Solo Project for Village",
      },
    },

    // ==========================================
    // 20. AI PACMAN - MULTI-AGENT SEARCH
    // ==========================================
    {
      id: "20",
      name: "ðŸŽ® AI Pacman - Multi-Agent Search",
      url:
        "https://github.com/Laxman824/Projects-Assignments/tree/main/AI-Pacman%20heuristics",
      description:
        "Advanced AI for Pacman featuring DFS, BFS, A*, and minimax with alpha-beta pruning.",
      languages: [
        { name: "Python", iconifyClass: "logos:python" },
        { name: "AI/ML", iconifyClass: "carbon:machine-learning-model" },
        { name: "Algorithms", iconifyClass: "carbon:flow" },
        { name: "Game AI", iconifyClass: "mdi:gamepad-variant" },
      ],
      details: {
        problem:
          "UC Berkeley's AI course project: implement various search algorithms and adversarial agents for Pacman, demonstrating understanding of AI fundamentals.",
        solution:
          "Implemented complete suite: uninformed search (DFS, BFS), informed search (A*, Greedy), adversarial search (Minimax, Alpha-Beta, Expectimax), and custom evaluation functions.",
        impact: [
          "Full score on all autograder tests",
          "Optimal pathfinding implementation",
          "Multi-ghost adversarial handling",
          "Top performance in class",
        ],
        features: [
          "Depth-First Search (DFS)",
          "Breadth-First Search (BFS)",
          "Uniform Cost Search",
          "A* with heuristics",
          "Minimax adversarial search",
          "Alpha-Beta pruning optimization",
          "Expectimax for stochastic agents",
          "Custom evaluation functions",
        ],
        techStack: {
          language: ["Python"],
          concepts: ["Graph Search", "Game Theory", "Heuristics"],
          framework: ["UC Berkeley Pacman"],
        },
        screenshots: [
          { title: "Pacman Maze", url: "/images/projects/pacman-maze.png" },
          {
            title: "Search Visualization",
            url: "/images/projects/pacman-search.png",
          },
        ],
        codeSnippets: [
          {
            filename: "minimax_agent.py",
            language: "python",
            code: `class MinimaxAgent:
    def __init__(self, depth=2):
        self.depth = depth
    
    def get_action(self, game_state):
        def minimax(state, depth, agent_index):
            # Terminal state or depth limit
            if state.is_win() or state.is_lose() or depth == 0:
                return self.evaluate(state), None
            
            num_agents = state.get_num_agents()
            next_agent = (agent_index + 1) % num_agents
            next_depth = depth - 1 if next_agent == 0 else depth
            
            if agent_index == 0:  # Pacman (maximizer)
                best_value = float('-inf')
                best_action = None
                for action in state.get_legal_actions(0):
                    successor = state.generate_successor(0, action)
                    value, _ = minimax(successor, next_depth, next_agent)
                    if value > best_value:
                        best_value = value
                        best_action = action
                return best_value, best_action
            
            else:  # Ghosts (minimizers)
                best_value = float('inf')
                for action in state.get_legal_actions(agent_index):
                    successor = state.generate_successor(agent_index, action)
                    value, _ = minimax(successor, next_depth, next_agent)
                    best_value = min(best_value, value)
                return best_value, None
        
        _, action = minimax(game_state, self.depth, 0)
        return action`,
          },
        ],
        learnings: [
          "Search algorithm implementations",
          "Game tree evaluation",
          "Heuristic design for A*",
          "Alpha-beta pruning optimization",
        ],
        timeline: "October 2020",
        role: "Developer",
        team: "AI Course Assignment",
      },
    },
    // ==========================================
    // 22. VM SNAPSHOT SYSTEM
    // ==========================================
    {
      id: "22",
      name: "ðŸ’¾ VM Snapshot System",
      url: "https://github.com/Laxman824/LiveSnapshotProject",
      description:
        "KVM-based VM snapshot service built with Rust. Features live snapshotting, incremental backups, and a web interface for VM state management.",
      languages: [
        { name: "Rust", iconifyClass: "logos:rust" },
        { name: "Virtualization", iconifyClass: "simple-icons:vmware" },
        { name: "HTML", iconifyClass: "logos:html-5" },
        { name: "CSS", iconifyClass: "logos:css-3" },
      ],
      details: {
        problem:
          "Cloud Computing course project: design and implement a reliable VM snapshotting service on top of a hypervisor stack. Needed to interact with KVM/libvirt, preserve VM state, and expose a usable management interface.",
        solution:
          "Implemented a Rust-based snapshot service that talks to KVM/libvirt to create and manage live snapshots of running VMs, with a lightweight web UI for creating, listing, and restoring snapshots without manual hypervisor commands.",
        impact: [
          "Enabled live snapshotting with minimal or no VM downtime.",
          "Introduced incremental snapshots to reduce storage footprint for frequent backups.",
          "Provided a simple web panel for operators instead of low-level CLI or hypervisor tools.",
          "Demonstrated safe systems programming in Rust for critical infrastructure workflows.",
        ],
        features: [
          "Live VM snapshotting while the guest OS keeps running.",
          "Full memory and disk state capture for consistent restore points.",
          "Incremental snapshot support to optimize storage usage.",
          "Snapshot listing, filtering, and metadata display in the UI.",
          "One-click snapshot restore from the web interface.",
          "Basic snapshot scheduling for periodic backups.",
          "Support for multiple VMs configured via libvirt domains.",
          "Storage space monitoring and simple retention strategy.",
        ],
        techStack: {
          backend: ["Rust", "libvirt-rs", "actix-web"],
          virtualization: ["KVM", "QEMU", "libvirt"],
          frontend: ["HTML", "CSS", "JavaScript"],
          storage: ["QCOW2", "LVM"],
        },
        screenshots: [
          {
            title: "Snapshot Manager Dashboard",
            url: "/images/projects/vm-manager.png",
          },
          {
            title: "Snapshot Restore Interface",
            url: "/images/projects/vm-restore.png",
          },
        ],
        codeSnippets: [
          {
            filename: "snapshot_service.rs",
            language: "rust",
            code: `use virt::connect::Connect;
use virt::domain::Domain;

pub struct SnapshotService {
    conn: Connect,
}

impl SnapshotService {
    pub fn new() -> Result<Self, Error> {
        let conn = Connect::open("qemu:///system")?;
        Ok(Self { conn })
    }

    pub fn create_snapshot(
        &self,
        domain_name: &str,
        snapshot_name: &str,
    ) -> Result<String, Error> {
        let domain = Domain::lookup_by_name(&self.conn, domain_name)?;

        // Build snapshot XML
        let xml = format!(r#"
            <domainsnapshot>
                <name>{}</name>
                <description>Created by VM Snapshot Service</description>
                <memory snapshot='internal'/>
                <disks>
                    <disk name='vda' snapshot='internal'/>
                </disks>
            </domainsnapshot>
        "#, snapshot_name);

        domain.snapshot_create_xml(&xml, 0)?;
        Ok(format!("Snapshot '{}' created for domain '{}'", snapshot_name, domain_name))
    }
}`,
          },
        ],
        learnings: [
          "Hands-on experience with KVM, QEMU, and libvirt APIs.",
          "Systems programming in Rust with focus on safety and error handling.",
          "Design of snapshot models including full and incremental backups.",
          "Practical understanding of VM storage backends and performance trade-offs.",
        ],
        timeline: "April 2023",
        role: "Backend & Systems Developer",
        team: "Cloud Computing Project (2 members)",
      },
    },

    // ==========================================
    // 21. SMART LIFT CONTROLLER SYSTEM
    // ==========================================
    {
      id: "21",
      name: "ðŸ¢ Smart Lift Controller System",
      url: "https://github.com/Laxman824/COL215-COMP-ARCH-assignments",
      description:
        "Intelligent elevator control system in Verilog with optimized scheduling for multi-floor buildings.",
      languages: [
        { name: "Verilog", iconifyClass: "simple-icons:v" },
        { name: "Logic Design", iconifyClass: "mdi:chip" },
        { name: "Simulation", iconifyClass: "mdi:elevator" },
        { name: "Algorithms", iconifyClass: "carbon:flow-data" },
      ],
      details: {
        problem:
          "Computer Architecture lab: design a digital system using Verilog. Elevator controller chosen for its state machine complexity and real-world applicability.",
        solution:
          "Designed a 3-floor elevator controller with optimized scheduling (SCAN algorithm), direction priority, door timing, emergency stop, and LED/seven-segment display output.",
        impact: [
          "Efficient SCAN scheduling algorithm",
          "Minimal wait time optimization",
          "Complete state machine implementation",
          "Synthesizable on FPGA",
        ],
        features: [
          "3-floor elevator simulation",
          "SCAN scheduling algorithm",
          "Direction priority handling",
          "Door open/close timing",
          "Emergency stop functionality",
          "Floor indicator display",
          "Request queue management",
          "Overload detection",
        ],
        techStack: {
          hdl: ["Verilog"],
          simulation: ["ModelSim", "Vivado"],
          fpga: ["Xilinx Artix-7"],
          tools: ["GTKWave"],
        },
        screenshots: [
          { title: "State Diagram", url: "/images/projects/lift-state.png" },
          { title: "Waveform", url: "/images/projects/lift-wave.png" },
        ],
        codeSnippets: [
          {
            filename: "lift_controller.v",
            language: "verilog",
            code: `module lift_controller(
    input clk,
    input reset,
    input [2:0] floor_request,  // 3 floors
    input [2:0] call_up,
    input [2:0] call_down,
    output reg [1:0] current_floor,
    output reg direction,  // 0=down, 1=up
    output reg door_open,
    output reg [6:0] seven_seg
);

    // States
    localparam IDLE = 2'b00;
    localparam MOVING = 2'b01;
    localparam DOOR_OPEN = 2'b10;
    
    reg [1:0] state;
    reg [3:0] door_timer;
    reg [2:0] pending_requests;
    
    always @(posedge clk or posedge reset) begin
        if (reset) begin
            current_floor <= 0;
            state <= IDLE;
            door_open <= 0;
        end else begin
            case (state)
                IDLE: begin
                    if (|pending_requests) begin
                        direction <= get_direction();
                        state <= MOVING;
                    end
                end
                
                MOVING: begin
                    if (should_stop()) begin
                        door_open <= 1;
                        door_timer <= 10;
                        state <= DOOR_OPEN;
                    end else begin
                        current_floor <= direction ? 
                            current_floor + 1 : 
                            current_floor - 1;
                    end
                end
                
                DOOR_OPEN: begin
                    if (door_timer == 0) begin
                        door_open <= 0;
                        state <= IDLE;
                    end else begin
                        door_timer <= door_timer - 1;
                    end
                end
            endcase
        end
    end
endmodule`,
          },
        ],
        learnings: [
          "Finite State Machine design",
          "Verilog HDL programming",
          "Digital logic optimization",
          "FPGA synthesis constraints",
        ],
        timeline: "November 2020",
        role: "Developer",
        team: "Computer Architecture Lab (2 members)",
      },
    },
  ],
};

// ============================================
// ðŸ“ž CONTACT PAGE
// ============================================
const contactPageData = {
  contactSection: {
    title: "Contact Me ðŸ’¬",
    profile_image_path: "laxman.png",
    description:
      "I'm always excited to discuss new opportunities, collaborate on interesting projects, or just chat about tech. Whether you have a question or just want to say hi, my inbox is always open!",
  },
  blogSection: {
    title: "Twitter",
    subtitle:
      "I occasionally write about AI, web development, and lessons learned from building production systems.",
    link: "https://twitter.com/laxmankethavat2",
    avatar_image_path: "blogs_image.svg",
  },
  calendlySection: {},
};

// ============================================
// â“ FAQ SECTION
// ============================================
const faqSection = {
  title: "Frequently Asked Questions",
  subtitle: "Common questions about my work and experience",
  faqs: [
    {
      question: "What technologies do you specialize in?",
      answer:
        "I specialize in Full Stack Development (React, Node.js, Python), AI/ML (LangChain, RAG systems, Computer Vision), and Cloud Architecture (GCP, AWS, Docker, Kubernetes). I'm particularly experienced in building production-ready GenAI applications.",
    },
    {
      question: "Are you open to new opportunities?",
      answer:
        "Yes! I'm always interested in challenging roles where I can apply my AI/ML and full-stack expertise to solve meaningful problems. Feel free to reach out via email or LinkedIn.",
    },
    {
      question: "What was your role at RAVI IIT Delhi?",
      answer:
        "At RAVI (Reading Assistant for Visually Impaired), I worked as a Software Engineer developing accessibility solutions using computer vision and ML technologies to help visually impaired students access STEM education.",
    },
    {
      question: "Do you take on freelance projects?",
      answer:
        "Selectively, yes. I'm particularly interested in AI integration projects, full-stack web applications, and computer vision solutions. Let's discuss your requirements!",
    },
    {
      question: "Can we collaborate on a project?",
      answer:
        "Absolutely! I'm always interested in collaborating on innovative projects. You can reach out to me through the contact form or via email at laxmankethavath5@gmail.com",
    },
  ],
};

// ============================================
// ðŸ§­ NAVIGATION
// ============================================
const navigation = {
  links: [
    { name: "Home", url: "/home" },
    { name: "Experience", url: "/experience" },
    { name: "Education", url: "/education" },
    { name: "Projects", url: "/projects" },
    { name: "FAQ", url: "/faq" },
    { name: "GitHub Stats", url: "/github" },
    { name: "Contact", url: "/contact" },
  ],
};

// ============================================
// ðŸ“¤ EXPORTS
// ============================================
export {
  settings,
  greeting,
  socialMediaLinks,
  skills,
  degrees,
  certifications,
  experience,
  projectsHeader,
  contactPageData,
  projects,
  navigation,
  faqSection,
};
